{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5dEgRpy3952M"
      },
      "outputs": [],
      "source": [
        "## Load libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "from keras.datasets import mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "plt.style.use('dark_background')\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "G9W_1_v_6yq7"
      },
      "outputs": [],
      "source": [
        "np.set_printoptions(precision=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4T7eUtw7Mh0z"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Q1e2N5S8MlCU"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'2.12.1'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbaFOIn_CDuN"
      },
      "source": [
        "---\n",
        "\n",
        "Mount Google Drive if running in Colab\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "02xk1yt7CE1D"
      },
      "outputs": [],
      "source": [
        "## Mount Google drive folder if running in Colab\n",
        "if('google.colab' in sys.modules):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount = True)\n",
        "    DIR = '/content/drive/MyDrive/Colab Notebooks/MAHE/MSIS Coursework/EvenSem2024MAHE'\n",
        "    DATA_DIR = DIR + '/Data/'\n",
        "    os.chdir(DIR)\n",
        "else:\n",
        "    DATA_DIR = 'Data/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16BpVeIWIOks"
      },
      "source": [
        "---\n",
        "\n",
        "Load diabetes data\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "E5kaKFKSIQgu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Diabetes dataset\n",
            "-----------\n",
            "Initial number of samples = 442\n",
            "Initial number of features = 11\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AGE</th>\n",
              "      <th>GENDER</th>\n",
              "      <th>BMI</th>\n",
              "      <th>BP</th>\n",
              "      <th>S1</th>\n",
              "      <th>S2</th>\n",
              "      <th>S3</th>\n",
              "      <th>S4</th>\n",
              "      <th>S5</th>\n",
              "      <th>S6</th>\n",
              "      <th>Y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>59</td>\n",
              "      <td>2</td>\n",
              "      <td>32.1</td>\n",
              "      <td>101.0</td>\n",
              "      <td>157</td>\n",
              "      <td>93.2</td>\n",
              "      <td>38.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.8598</td>\n",
              "      <td>87</td>\n",
              "      <td>151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>48</td>\n",
              "      <td>1</td>\n",
              "      <td>21.6</td>\n",
              "      <td>87.0</td>\n",
              "      <td>183</td>\n",
              "      <td>103.2</td>\n",
              "      <td>70.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.8918</td>\n",
              "      <td>69</td>\n",
              "      <td>75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>72</td>\n",
              "      <td>2</td>\n",
              "      <td>30.5</td>\n",
              "      <td>93.0</td>\n",
              "      <td>156</td>\n",
              "      <td>93.6</td>\n",
              "      <td>41.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.6728</td>\n",
              "      <td>85</td>\n",
              "      <td>141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>24</td>\n",
              "      <td>1</td>\n",
              "      <td>25.3</td>\n",
              "      <td>84.0</td>\n",
              "      <td>198</td>\n",
              "      <td>131.4</td>\n",
              "      <td>40.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.8903</td>\n",
              "      <td>89</td>\n",
              "      <td>206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "      <td>23.0</td>\n",
              "      <td>101.0</td>\n",
              "      <td>192</td>\n",
              "      <td>125.4</td>\n",
              "      <td>52.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.2905</td>\n",
              "      <td>80</td>\n",
              "      <td>135</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   AGE  GENDER   BMI     BP   S1     S2    S3   S4      S5  S6    Y\n",
              "0   59       2  32.1  101.0  157   93.2  38.0  4.0  4.8598  87  151\n",
              "1   48       1  21.6   87.0  183  103.2  70.0  3.0  3.8918  69   75\n",
              "2   72       2  30.5   93.0  156   93.6  41.0  4.0  4.6728  85  141\n",
              "3   24       1  25.3   84.0  198  131.4  40.0  5.0  4.8903  89  206\n",
              "4   50       1  23.0  101.0  192  125.4  52.0  4.0  4.2905  80  135"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Load diabetes data\n",
        "file = DATA_DIR+'diabetes_regression1.csv'\n",
        "df= pd.read_csv(file, header = 0)\n",
        "\n",
        "print('Diabetes dataset')\n",
        "print('-----------')\n",
        "print('Initial number of samples = %d'%(df.shape[0]))\n",
        "print('Initial number of features = %d\\n'%(df.shape[1]))\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "AJE5ehBOClXW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['GENDER']\n",
            "['AGE', 'BMI', 'BP', 'S1', 'S2', 'S3', 'S4', 'S5', 'S6', 'Y']\n"
          ]
        }
      ],
      "source": [
        "## Create lists of ordinal, categorical, and continuous features\n",
        "#categorical_features =  ['GENDER', 'BMILEVEL']\n",
        "categorical_features =  ['GENDER']\n",
        "continuous_features = df.drop(categorical_features, axis = 1).columns.tolist()\n",
        "print(categorical_features)\n",
        "print(continuous_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3NyP_EoDG1i"
      },
      "source": [
        "---\n",
        "\n",
        "Assign 'category' datatype to categorical columns\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-dVFfOBlDJ5n"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AGE         int64\n",
            "GENDER      int64\n",
            "BMI       float64\n",
            "BP        float64\n",
            "S1          int64\n",
            "S2        float64\n",
            "S3        float64\n",
            "S4        float64\n",
            "S5        float64\n",
            "S6          int64\n",
            "Y           int64\n",
            "dtype: object\n",
            "----\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "AGE          int64\n",
              "GENDER    category\n",
              "BMI        float64\n",
              "BP         float64\n",
              "S1           int64\n",
              "S2         float64\n",
              "S3         float64\n",
              "S4         float64\n",
              "S5         float64\n",
              "S6           int64\n",
              "Y            int64\n",
              "dtype: object"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Assign 'category' datatype to ordinal and categorical columns\n",
        "print(df.dtypes)\n",
        "df[categorical_features] = df[categorical_features].astype('category')\n",
        "print('----')\n",
        "df.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m95YNt2eDUJ8"
      },
      "source": [
        "---\n",
        "\n",
        "Remove the target variable column from the list of continuous features\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "XuQGzpefDUqE"
      },
      "outputs": [],
      "source": [
        "## Remove the target variable column from the list of continuous features\n",
        "continuous_features.remove('Y')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "tn-qjRocE8Lj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Diabetes data set\n",
            "---------------------\n",
            "Number of training samples = 10\n",
            "Number of features = 353\n"
          ]
        }
      ],
      "source": [
        "## Train and test split of the data\n",
        "X = df.drop('Y', axis = 1)\n",
        "y = df['Y']\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n",
        "\n",
        "num_features = X_train.shape[0]\n",
        "num_samples = X_train.shape[1]\n",
        "\n",
        "print('Diabetes data set')\n",
        "print('---------------------')\n",
        "print('Number of training samples = %d'%(num_samples))\n",
        "print('Number of features = %d'%(num_features))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKqSu9e7G-fh"
      },
      "source": [
        "---\n",
        "\n",
        "Build pipeline for categorical and continuous features\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "FKAqkz1GHGvm"
      },
      "outputs": [],
      "source": [
        "## Build pipeline for categorical and continuous features\n",
        "\n",
        "# Pipeline object for categorical (features\n",
        "categorical_transformer = Pipeline(steps = [('onehotenc', OneHotEncoder(handle_unknown = 'ignore'))])\n",
        "\n",
        "# Pipeline object for continuous features\n",
        "continuous_transformer = Pipeline(steps = [('scaler', RobustScaler())])\n",
        "\n",
        "# Create a preprocessor object for all features\n",
        "preprocessor = ColumnTransformer(transformers = [('continuous', continuous_transformer, continuous_features),\n",
        "                                                 ('categorical', categorical_transformer, categorical_features)\n",
        "                                                ],\n",
        "                                 remainder = 'passthrough'\n",
        "                                 )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUWtfRseHv2O"
      },
      "source": [
        "---\n",
        "\n",
        "Fit and transform train data using preprocessor followed by transforming test data\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "RvrUIw4SHzO_"
      },
      "outputs": [],
      "source": [
        "## Fit and transform train data using preprocessor\n",
        "X_train_transformed = preprocessor.fit_transform(X_train).T\n",
        "# Update number of features\n",
        "num_features = X_train_transformed.shape[0]\n",
        "# Transform training data using preprocessor\n",
        "X_test_transformed = preprocessor.transform(X_test).T\n",
        "# Convert Y_train and Y_test to numpy arrays\n",
        "Y_train = Y_train.to_numpy()\n",
        "Y_test = Y_test.to_numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYn_hOccDa3M"
      },
      "source": [
        "---\n",
        "\n",
        "A generic layer class with forward and backward methods\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "N4pKUhCyMrWm"
      },
      "outputs": [],
      "source": [
        "class Layer:\n",
        "  def __init__(self):\n",
        "    self.input = None\n",
        "    self.output = None\n",
        "\n",
        "  def forward(self, input):\n",
        "    pass\n",
        "\n",
        "  def backward(self, output_gradient, learning_rate):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMt81Faf9-bf"
      },
      "source": [
        "---\n",
        "\n",
        "Mean squared error (MSE) loss and its gradient\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "hdXSGW2s7zKd"
      },
      "outputs": [],
      "source": [
        "## Define the loss function and its gradient\n",
        "def mse(Y, Yhat):\n",
        "  return(np.mean(0.5*(Y - Yhat)**2))\n",
        "  #TensorFlow in-built function for mean squared error loss\n",
        "  #mse = tf.keras.losses.MeanSquaredError()\n",
        "  #mse(Y, Yhat).numpy()\n",
        "\n",
        "def mse_gradient(Y, Yhat):\n",
        "  return(Yhat - Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmcNJTjS-BaW"
      },
      "source": [
        "---\n",
        "\n",
        "Generic activation layer class\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C21FcWIEwGCN"
      },
      "outputs": [],
      "source": [
        "class Activation(Layer):\n",
        "    def __init__(self, activation, activation_gradient):\n",
        "        self.activation = activation\n",
        "        self.activation_gradient = activation_gradient\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.input = input\n",
        "        self.output = self.activation(self.input)\n",
        "        return(self.output)\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate = None):\n",
        "        return(output_gradient[:-1, :] * self.activation_gradient(self.input))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JheGWSoKxYWu"
      },
      "source": [
        "---\n",
        "\n",
        "Specific activation layer classes\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQ5ybz_Yxbef"
      },
      "outputs": [],
      "source": [
        "class Sigmoid(Activation):\n",
        "    def __init__(self):\n",
        "        def sigmoid(z):\n",
        "            return 1 / (1 + np.exp(-z))\n",
        "\n",
        "        def sigmoid_gradient(z):\n",
        "            a = sigmoid(z)\n",
        "            return a * (1 - a)\n",
        "\n",
        "        super().__init__(sigmoid, sigmoid_gradient)\n",
        "\n",
        "class Tanh(Activation):\n",
        "    def __init__(self):\n",
        "        def tanh(z):\n",
        "            return np.tanh(z)\n",
        "\n",
        "        def tanh_gradient(z):\n",
        "            a = np.tanh(z)\n",
        "            return 1 - a**2\n",
        "\n",
        "        super().__init__(tanh, tanh_gradient)\n",
        "\n",
        "class ReLU(Activation):\n",
        "    def __init__(self):\n",
        "        def relu(z):\n",
        "            return z * (z > 0)\n",
        "\n",
        "        def relu_gradient(z):\n",
        "            return 1. * (z > 0)\n",
        "\n",
        "        super().__init__(relu, relu_gradient)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKnqi7rf-MBn"
      },
      "source": [
        "---\n",
        "\n",
        "Dense layer class\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ctXhZYCTmHK"
      },
      "outputs": [],
      "source": [
        "## Dense layer class\n",
        "class Dense(Layer):\n",
        "    def __init__(self, input_size, output_size, reg_strength):\n",
        "        self.weights = 0.01*np.random.randn(output_size, input_size+1) # bias trick\n",
        "        self.weights[:, -1] = 0.01 # set all bias values to the same nonzero constant\n",
        "        self.reg_strength = reg_strength\n",
        "        self.reg_loss = None\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.input = np.vstack([input, np.ones((1, input.shape[1]))]) # bias trick\n",
        "        self.output= np.dot(self.weights, self.input)\n",
        "        # Calculate regularization loss\n",
        "        self.reg_loss = self.reg_strength * np.sum(self.weights[:, :-1] * self.weights[:, :-1])\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        ## Following is the inefficient way of calculating the backward gradient\n",
        "        #weights_gradient = np.zeros((self.output.shape[0], self.input.shape[0]), dtype = np.float64)\n",
        "        #for b in range(output_gradient.shape[1]):\n",
        "        #  weights_gradient += np.dot(output_gradient[:, b].reshape(-1, 1), self.input[:, b].reshape(-1, 1).T)\n",
        "        #weights_gradient = (1/output_gradient.shape[1])*weights_gradient\n",
        "\n",
        "        ## Following is the efficient way of calculating the weights gradient w.r.t. data\n",
        "        weights_gradient = (1/output_gradient.shape[1])*np.dot(np.atleast_2d(output_gradient), np.atleast_2d(self.input).T)\n",
        "        # Add the regularization gradient here\n",
        "        weights_gradient += 2 * self.reg_strength * np.hstack([self.weights[:, :-1], np.zeros((self.weights.shape[0], 1))])\n",
        "\n",
        "\n",
        "        input_gradient = np.dot(self.weights.T, output_gradient)\n",
        "        self.weights = self.weights + learning_rate * (-weights_gradient)\n",
        "\n",
        "        return(input_gradient)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2W1howeOJegI"
      },
      "source": [
        "---\n",
        "\n",
        "Function to generate sample indices for batch processing according to batch size\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHyjEf22IRpc"
      },
      "outputs": [],
      "source": [
        "## Function to generate sample indices for batch processing according to batch size\n",
        "def generate_batch_indices(num_samples, batch_size):\n",
        "  # Reorder sample indices\n",
        "  reordered_sample_indices = np.random.choice(num_samples, num_samples, replace = False)\n",
        "  # Generate batch indices for batch processing\n",
        "  batch_indices = np.split(reordered_sample_indices, np.arange(batch_size, len(reordered_sample_indices), batch_size))\n",
        "  return(batch_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fI_Gms9fJqbs"
      },
      "source": [
        "---\n",
        "\n",
        "Train the 3-layer neural network (8/8/1 structure) using batch training with batch size = 16\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGIzrN-rPuI4"
      },
      "outputs": [],
      "source": [
        "## Train the 2-hidden layer neural network (8 nodes, 8 nodes followed by 1 node)\n",
        "## using batch training with batch size = 100\n",
        "learning_rate = 1e-04 # learning rate\n",
        "batch_size = 32 # batch size\n",
        "nepochs = 10000 # number of epochs\n",
        "reg_strength = 1.0 # regularization strength\n",
        "# Create empty array to store training losses over each epoch\n",
        "loss_train_epoch = np.empty(nepochs, dtype = np.float64)\n",
        "# Create empty array to store test losses over each epoch\n",
        "loss_test_epoch = np.empty(nepochs, dtype = np.float64)\n",
        "\n",
        "\n",
        "# Neural network architecture\n",
        "\n",
        "dlayer1 = Dense(num_features, 16, reg_strength) # define dense layer 1\n",
        "alayer1 = Tanh() # ReLU activation layer 1\n",
        "dlayer2 = Dense(16, 1, reg_strength) # define dense layer 2\n",
        "\n",
        "\n",
        "# Steps: run over each sample in the batch, calculate loss, gradient of loss,\n",
        "# and update weights.\n",
        "\n",
        "epoch = 0\n",
        "while epoch < nepochs:\n",
        "  batch_indices = generate_batch_indices(num_samples, batch_size)\n",
        "  loss = 0\n",
        "  for b in range(len(batch_indices)):\n",
        "    # Forward propagation for training data\n",
        "    dlayer1.forward(X_train_transformed[:, batch_indices[b]]) # forward prop dense layer 1 with batch feature added\n",
        "    alayer1.forward(dlayer1.output) # forward prop activation layer 1\n",
        "    dlayer2.forward(alayer1.output) # forward prop dense layer 2\n",
        "    # Calculate training data loss\n",
        "    loss += mse(Y_train[batch_indices[b]], dlayer2.output)\n",
        "    # Add the regularization losses\n",
        "    loss += dlayer1.reg_loss + dlayer2.reg_loss\n",
        "\n",
        "    # Backward prop starts here\n",
        "    grad = mse_gradient(Y_train[batch_indices[b]], dlayer2.output)\n",
        "    grad = dlayer2.backward(grad, learning_rate)\n",
        "    grad = alayer1.backward(grad)\n",
        "    grad = dlayer1.backward(grad, learning_rate)\n",
        "  # Calculate the average training loss for the current epoch\n",
        "  loss_train_epoch[epoch] = loss/len(batch_indices)\n",
        "\n",
        "  # Forward propagation for test data\n",
        "  dlayer1.forward(X_test_transformed)\n",
        "  alayer1.forward(dlayer1.output)\n",
        "  dlayer2.forward(alayer1.output)\n",
        "\n",
        "  # Calculate test data loss plus regularization loss\n",
        "  loss_test_epoch[epoch] =  mse(Y_test, dlayer2.output) + dlayer1.reg_loss + dlayer2.reg_loss\n",
        "\n",
        "  print('Epoch %d: train loss = %f, test loss = %f'%(epoch+1, loss_train_epoch[epoch], loss_test_epoch[epoch]))\n",
        "  epoch = epoch + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhXEFASk-Tkv"
      },
      "source": [
        "---\n",
        "\n",
        "Plot training loss vs. epoch\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iv3k23SlCqGf"
      },
      "outputs": [],
      "source": [
        "# Plot train and test loss as a function of epoch:\n",
        "fig, ax = plt.subplots(1, 1, figsize = (4, 4))\n",
        "fig.tight_layout(pad = 4.0)\n",
        "ax.plot(loss_train_epoch, 'b', label = 'Train')\n",
        "ax.plot(loss_test_epoch, 'r', label = 'Test')\n",
        "ax.set_xlabel('Epoch', fontsize = 12)\n",
        "ax.set_ylabel('Loss value', fontsize = 12)\n",
        "ax.legend()\n",
        "ax.set_title('Loss vs. Epoch for reg. strength 1..0', fontsize = 14);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaLoOOWK-WBj"
      },
      "source": [
        "---\n",
        "\n",
        "Test performance on test data\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7AEbmpcKcPY"
      },
      "outputs": [],
      "source": [
        "dlayer1.forward(X_test_transformed)\n",
        "alayer1.forward(dlayer1.output)\n",
        "dlayer2.forward(alayer1.output)\n",
        "ypred = dlayer2.output.flatten()\n",
        "ytrue = Y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EL88EDPu2cKU"
      },
      "source": [
        "---\n",
        "\n",
        "Define neural network architecture for regression\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_YHFR9c2Zqq"
      },
      "outputs": [],
      "source": [
        "# Define neural network architecture for regression\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(16, activation='relu', input_shape=(X_train_transformed.T.shape[1], ), kernel_regularizer = keras.regularizers.l2(l = 1.0)),\n",
        "    layers.Dense(1)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlNvESd_2eyX"
      },
      "source": [
        "---\n",
        "\n",
        "Compile the neural network\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVRF1u3i2hcA"
      },
      "outputs": [],
      "source": [
        "# Compile the neural network model\n",
        "opt = tf.keras.optimizers.Adam(learning_rate = 1e-04)\n",
        "model.compile(optimizer = opt, loss = 'mean_squared_error')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGVN5fi62mHY"
      },
      "source": [
        "---\n",
        "\n",
        "Train the model\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPdXEpps2k5G"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "history = model.fit(X_train_transformed.T, Y_train, epochs = 10000, batch_size = 32, validation_data=(X_test_transformed.T, Y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d74Ke_D9CXc"
      },
      "source": [
        "---\n",
        "\n",
        "Plot train and test loss as a function of epoch\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTvGrHgF9071"
      },
      "outputs": [],
      "source": [
        "# Plot train and test loss as a function of epoch:\n",
        "fig, ax = plt.subplots(1, 1, figsize = (4, 4))\n",
        "fig.tight_layout(pad = 4.0)\n",
        "ax.plot( history.history['loss'], 'b', label = 'Train')\n",
        "ax.plot( history.history['val_loss'], 'r', label = 'Test')\n",
        "ax.set_xlabel('Epoch', fontsize = 12)\n",
        "ax.set_ylabel('Loss value', fontsize = 12)\n",
        "ax.legend()\n",
        "ax.set_title('Loss vs. Epoch for reg. strength 1.0', fontsize = 14);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtOAXO2n9GMr"
      },
      "source": [
        "---\n",
        "\n",
        "Compare the true and predicted values\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qz9ORVFV3HPz"
      },
      "outputs": [],
      "source": [
        "## Compare the true and predicted values\n",
        "ypred_TF = model.predict(X_test_transformed.T)\n",
        "np.column_stack((Y_test, ypred, ypred_TF))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
