{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5dEgRpy3952M"
      },
      "outputs": [],
      "source": [
        "## Load libraries\n",
        "import numpy as np\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "plt.style.use('dark_background')\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "G9W_1_v_6yq7"
      },
      "outputs": [],
      "source": [
        "np.set_printoptions(precision=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4T7eUtw7Mh0z"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\adity\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Q1e2N5S8MlCU"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'2.15.0'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "B4EjOi-OM4Gp"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'np' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m num_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;66;03m# number of output labels\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Data matrix (each column = single sample)\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m10\u001b[39m), size \u001b[38;5;241m=\u001b[39m (num_features, num_samples), replace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Class labels\u001b[39;00m\n\u001b[0;32m      9\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m], size \u001b[38;5;241m=\u001b[39m num_samples, replace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ],
      "source": [
        "# Generate artificial data with 64 samples, 5 features per sample\n",
        "# and 3 output classes\n",
        "num_samples = 64 # number of samples\n",
        "num_features = 5 # number of features (a.k.a. dimensionality)\n",
        "num_labels = 3 # number of output labels\n",
        "# Data matrix (each column = single sample)\n",
        "X = np.random.choice(np.arange(3, 10), size = (num_features, num_samples), replace = True)\n",
        "# Class labels\n",
        "y = np.random.choice([0, 1, 2], size = num_samples, replace = True)\n",
        "# One-hot encode class labels\n",
        "y = tf.keras.utils.to_categorical(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrXipxwrJ0_8"
      },
      "source": [
        "---\n",
        "\n",
        "A generic layer class with forward and backward methods\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "N4pKUhCyMrWm"
      },
      "outputs": [],
      "source": [
        "class Layer:\n",
        "  def __init__(self):\n",
        "    self.input = None\n",
        "    self.output = None\n",
        "\n",
        "  def forward(self, input):\n",
        "    pass\n",
        "\n",
        "  def backward(self, output_gradient, learning_rate):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdLfiQSlOSUU"
      },
      "source": [
        "---\n",
        "\n",
        "The softmax classifier steps for a generic sample $\\mathbf{x}$ with (one-hot encoded) true label $\\mathbf{y}$ (3 possible categories) using a randomly initialized weights matrix (with bias abosrbed as its last last column):\n",
        "\n",
        "1. Calculate raw scores vector for a generic sample $\\mathbf{x}$  (bias feature added): $$\\mathbf{z} = \\mathbf{Wx}.$$\n",
        "2. Calculate softmax probabilities (that is, softmax-activate the raw scores) $$\\mathbf{a} = \\text{softmax}(\\mathbf{z})\\Rightarrow\\begin{bmatrix}a_0\\\\a_1\\\\a_2\\end{bmatrix}= \\text{softmax}\\left(\\begin{bmatrix}z_0\\\\z_1\\\\z_2\\end{bmatrix}\\right)=\\begin{bmatrix}\\frac{e^{z_0}}{e^{z_0}+e^{z_1}+e^{z_2}}\\\\\\frac{e^{z_1}}{e^{z_0}+e^{z_1}+e^{z_2}}\\\\\\frac{e^{z_2}}{e^{z_0}+e^{z_1}+e^{z_2}}\\end{bmatrix}$$\n",
        "3. Softmax loss for this sample is (where output label $y$ is not yet one-hot encoded)\n",
        "$$\\begin{align*}L &=  -\\log([\\mathbf{a}]_y) \\\\&= -\\log\\left(\\left[\\text{softmax}(\\mathbf{z})\\right]_y\\right)\\\\ &= -\\log\\left(\\left[\\text{softmax}(\\mathbf{Wx})\\right]_y\\right).\\end{align*}$$\n",
        "4. Predicted probability vector that the sample belongs to each one of the output categories is given a new name $$\\hat{\\mathbf{y}} = \\mathbf{a}.$$\n",
        "5. One-hot encoding the output label $$\\underbrace{y\\rightarrow\\mathbf{y}}_{\\text{e.g.}\\,2\\,\\rightarrow\\begin{bmatrix}0\\\\0\\\\1\\end{bmatrix}}$$ results in the following representation for the softmax loss for the sample which is also referred to as the categorical crossentropy (CCE) loss:\n",
        "$$\\begin{align*}L &= L\\left(\\mathbf{y},\\hat{\\mathbf{y}}\\right)=\\sum_{k=0}^2-y_k\\log\\left(\\hat{y}_k\\right)\\end{align*}.$$\n",
        "5. Calculate the gradient of the loss for the sample w.r.t. weights by following the computation graph from top to bottom (that is, backward):\n",
        "$$\\begin{align*} L\\\\{\\color{yellow}\\downarrow}\\\\ \\hat{\\mathbf{y}} &= \\mathbf{a}\\\\{\\color{yellow}\\downarrow}\\\\\\mathbf{z}\\\\{\\color{yellow}\\downarrow}\\\\\\mathbf{W}\\end{align*}$$\n",
        "$$\\begin{align*}\\Rightarrow \\nabla_\\mathbf{W}(L) &= \\nabla_\\mathbf{W}(\\mathbf{z}) \\times\\nabla_\\mathbf{z}(\\hat{\\mathbf{y}})\\times\\nabla_{\\hat{\\mathbf{y}}}(L)\\\\&= \\underbrace{\\nabla_\\mathbf{W}(\\mathbf{z})}_\\text{first term} \\times\\underbrace{\\nabla_\\mathbf{z}(\\mathbf{a})}_\\text{second to last term}\\times\\underbrace{\\nabla_\\hat{\\mathbf{y}}(L)}_\\text{last term}.\\end{align*}$$\n",
        "7. Now focus on the last term $\\nabla_\\hat{\\mathbf{y}}(L)$:\n",
        "$$\\begin{align*}\\nabla_\\hat{\\mathbf{y}}(L) &=\\begin{bmatrix}\\nabla_{\\hat{y}_0}(L)\\\\\\nabla_{\\hat{y}_1}(L)\\\\\\nabla_{\\hat{y}_2}(L)\\end{bmatrix} = \\begin{bmatrix}-y_0/\\hat{y}_0\\\\-y_1/\\hat{y}_1\\\\-y_2/\\hat{y}_2\\end{bmatrix}.\\end{align*}$$\n",
        "8. Now focus on the second to last term $\\nabla_\\mathbf{z}(\\mathbf{a})$:\n",
        "$$\\begin{align*}\\nabla_\\mathbf{z}(\\mathbf{a}) &= \\nabla_\\mathbf{z}\\left(\\begin{bmatrix}a_0\\\\a_1\\\\a_2\\end{bmatrix}\\right)\\\\ &= \\begin{bmatrix}\\nabla_\\mathbf{z}(a_0)&\\nabla_\\mathbf{z}(a_1)&\\nabla_\\mathbf{z}(a_2)\\end{bmatrix} \\\\&= \\begin{bmatrix}\\nabla_{z_0}(a_0)&\\nabla_{z_0}(a_1)&\\nabla_{z_0}(a_2)\\\\\\nabla_{z_1}(a_0)&\\nabla_{z_1}(a_1)&\\nabla_{z_1}(a_2)\\\\\\nabla_{z_2}(a_0)&\\nabla_{z_2}(a_1)&\\nabla_{z_2}(a_2)\\end{bmatrix}\\\\&=\\begin{bmatrix}a_0(1-a_0)&-a_1a_0&-a_2a_0\\\\-a_0a_1&a_1(1-a_1)&-a_2a_1\\\\-a_0a_2&-a_1a_2&a_2(1-a_2)\\end{bmatrix}.\\end{align*}$$\n",
        "9. Now focus on the last term $\\nabla_\\mathbf{W}(\\mathbf{z}) = \\nabla_\\mathbf{W}(\\mathbf{Wx})$:\n",
        "\n",
        "![](https://onedrive.live.com/embed?resid=37720F927B6DDC34%21103155&authkey=%21AMH79mXBdb_raAA&width=660)\n",
        "\n",
        "The full gradient can be written as $\\nabla_\\mathbf{W}(L)=$\n",
        "\n",
        "![](https://onedrive.live.com/embed?resid=37720F927B6DDC34%21103156&authkey=%21AIdyOQ3a-er-7-A&width=660)\n",
        "\n",
        "$$\\begin{align*}=\\begin{bmatrix}a_1(1-a_1)&-a_2a_1&-a_3a_1\\\\-a_1a_2&a_2(1-a_2)&-a_3a_2\\\\-a_1a_3&-a_2a_3&a_3(1-a_3)\\end{bmatrix}\\times\\begin{bmatrix}-y_1/\\hat{y}_1\\\\-y_2/\\hat{y}_2\\\\-y_3/\\hat{y}_3\\end{bmatrix}\\mathbf{x}^\\mathrm{T}.\\end{align*}$$\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9YGwzbz72CZ"
      },
      "source": [
        "---\n",
        "\n",
        "CCE loss and its gradient\n",
        "\n",
        "$$\\begin{align*}L &= L\\left(\\mathbf{y},\\hat{\\mathbf{y}}\\right)=\\sum_{k=0}^2-y_k\\log\\left(\\hat{y}_k\\right)\\\\\\nabla_\\hat{\\mathbf{y}}(L) &=\\begin{bmatrix}\\nabla_{\\hat{y}_0}(L)\\\\\\nabla_{\\hat{y}_1}(L)\\\\\\nabla_{\\hat{y}_2}(L)\\end{bmatrix} = \\begin{bmatrix}-y_0/\\hat{y}_0\\\\-y_1/\\hat{y}_1\\\\-y_2/\\hat{y}_2\\end{bmatrix}.\\end{align*}$$\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdXSGW2s7zKd"
      },
      "outputs": [],
      "source": [
        "## Define the loss function and its gradient\n",
        "def cce(y, yhat):\n",
        "  return(?)\n",
        "\n",
        "def cce_gradient(y, yhat):\n",
        "  return(?)\n",
        "\n",
        "# TensorFlow in-built function for categorical crossentropy loss\n",
        "#cce = tf.keras.losses.CategoricalCrossentropy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smgXLg9p65HV"
      },
      "source": [
        "---\n",
        "\n",
        "Softmax activation layer class\n",
        "$$\\begin{align*}\\text{forward:}\\ \\mathbf{a} &=\\text{softmax}(\\mathbf{z}),\\\\\\text{backward:}\\ \\nabla_\\mathbf{z}(L) &= \\nabla_{\\mathbf{z}}(\\mathbf{a})\\times\\nabla_{\\mathbf{a}}(L) = \\nabla_{\\mathbf{z}}(\\mathbf{a})\\times\\nabla_{\\hat{\\mathbf{y}}}(L)\\\\&=\\begin{bmatrix}a_0(1-a_0)&-a_1a_0&-a_2a_0\\\\-a_0a_1&a_1(1-a_1)&-a_2a_1\\\\-a_0a_2&-a_1a_2&a_2(1-a_2)\\end{bmatrix}\\begin{bmatrix}-y_0/\\hat{y}_0\\\\-y_1/\\hat{y}_1\\\\-y_2/\\hat{y}_2\\end{bmatrix}.\\end{align*}$$\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4x1Xn3AbJlNy"
      },
      "outputs": [],
      "source": [
        "## Softmax activation layer class\n",
        "class Softmax(Layer):\n",
        "  def forward(self, input):\n",
        "    self.output = ?\n",
        "\n",
        "  def backward(self, output_gradient, learning_rate = None):\n",
        "    T = ?\n",
        "    return(np.einsum(?, T, output_gradient))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkPFfd1U68dj"
      },
      "source": [
        "---\n",
        "\n",
        "Dense layer class\n",
        "\n",
        "$$\\begin{align*}\\text{forward:}\\ \\mathbf{z}&=\\mathbf{Wx}\\\\\\text{backward:}\\ \\nabla_\\mathbf{W}(L)&=\\nabla_{\\mathbf{W}}(\\mathbf{z})\\times\\nabla_{\\mathbf{z}}(L)\\\\&=\\nabla_{\\mathbf{z}}(L)\\mathbf{x}^\\mathrm{T}.\\end{align*}$$\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ctXhZYCTmHK"
      },
      "outputs": [],
      "source": [
        "## Dense layer class\n",
        "class Dense(Layer):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.weights = np.random.randn(output_size, input_size+1) # bias trick\n",
        "        self.weights[:, -1] = 0.01 # Set all bias values to the same nonzero constant\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.input = np.vstack([input, np.ones((1, input.shape[1]))]) # bias trick\n",
        "        self.output= np.dot(self.weights, self.input)\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        weights_gradient = np.dot(?, ?)\n",
        "        input_gradient = np.dot(self.weights.T, output_gradient)\n",
        "        self.weights = self.weights + learning_rate * (-weights_gradient)\n",
        "        return(input_gradient)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPVu8RC5Tvmy"
      },
      "outputs": [],
      "source": [
        "# Forward and backward propagation for a batch size 16 of samples\n",
        "batch_size = 16\n",
        "learning_rate = 1e-02 # learning rate\n",
        "dlayer = Dense(num_features, num_labels) # define dense layer\n",
        "print(dlayer.weights)\n",
        "dlayer.forward(X[:, 0:batch_size]) # forward prop\n",
        "softmax = Softmax() # define softmax activation\n",
        "softmax.forward(dlayer.output) # Softmax activate\n",
        "loss = cce(y[:, 0:batch_size], softmax.output) # forward prop is over\n",
        "print(loss)\n",
        "grad = cce_gradient(y[:, 0:batch_size], softmax.output)\n",
        "grad = softmax.backward(grad)\n",
        "grad = dlayer.backward((1/batch_size)*grad, learning_rate)\n",
        "print('-----')\n",
        "print(dlayer.weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2W1howeOJegI"
      },
      "source": [
        "---\n",
        "\n",
        "Function to generate sample indices for batch processing according to batch size\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHyjEf22IRpc"
      },
      "outputs": [],
      "source": [
        "## Function to generate sample indices for batch processing according to batch size\n",
        "def generate_batch_indices(num_samples, batch_size):\n",
        "  # Reorder sample indices\n",
        "  reordered_sample_indices = np.random.choice(num_samples, num_samples, replace = False)\n",
        "  # Generate batch indices for batch processing\n",
        "  batch_indices = np.split(reordered_sample_indices, np.arange(batch_size, len(reordered_sample_indices), batch_size))\n",
        "  return(batch_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKFmCaFsJhkR"
      },
      "source": [
        "---\n",
        "\n",
        "Example generation of batch indices\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9QwikN0IYSp"
      },
      "outputs": [],
      "source": [
        "## Example generation of batch indices\n",
        "# Batch size\n",
        "batch_size = 16\n",
        "# Number of batches per epoch\n",
        "num_iterations_per_epoch = int(np.ceil(num_samples/batch_size))\n",
        "print('Number of iterations per epoch = %d\\n'%(num_iterations_per_epoch))\n",
        "b = 0\n",
        "epoch = 0\n",
        "for it in range(num_iterations_per_epoch):\n",
        "  if it % num_iterations_per_epoch == 0:# check if we are at the start of an epoch\n",
        "    print('--------------------------------')\n",
        "    print('Epoch %d:'%(epoch+1))\n",
        "    batch_indices = generate_batch_indices(num_samples, batch_size)\n",
        "    b = 0\n",
        "    epoch = epoch + 1\n",
        "    print('--------------------------------')\n",
        "  print('In iteration %d, using samples' % (it+1))\n",
        "  print(batch_indices[b])\n",
        "  b += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fI_Gms9fJqbs"
      },
      "source": [
        "---\n",
        "\n",
        "Train the 0-layer neural network using batch training with batch size = 16\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGIzrN-rPuI4"
      },
      "outputs": [],
      "source": [
        "## Train the 0-layer neural network using batch training with batch size = 16\n",
        "\n",
        "learning_rate = 1e-04 # learning rate\n",
        "batch_size = 16 # batch size\n",
        "nepochs = 100 # number of epochs\n",
        "\n",
        "loss = 0 # initialize loss\n",
        "dlayer = Dense(num_features, num_labels) # define dense layer\n",
        "softmax = Softmax() # define softmax activation layer\n",
        "\n",
        "# Steps: run over each sample in the batch, calculate loss, gradient of loss,\n",
        "# and update weights.\n",
        "\n",
        "# Number of batches per epoch\n",
        "num_iterations_per_epoch = int(np.ceil(num_samples/batch_size))\n",
        "print('Number of iterations per epoch = %d\\n'%(num_iterations_per_epoch))\n",
        "b = 0\n",
        "epoch = 0\n",
        "while epoch <= nepochs:\n",
        "  for it in range(num_iterations_per_epoch):\n",
        "    if it % num_iterations_per_epoch == 0:# check if we are at the start of an epoch\n",
        "      print('Epoch %d:, loss = %f'%(epoch+1, loss))\n",
        "      batch_indices = generate_batch_indices(num_samples, batch_size)\n",
        "      dlayer.forward(X[:, ?]) # forward prop\n",
        "      softmax.forward(dlayer.output) # Softmax activate\n",
        "      loss += cce(y[:, ?], ?) # calculate loss for sample\n",
        "      # Backward prop starts here\n",
        "      grad = cce_gradient(?, ?)\n",
        "      grad = softmax.backward(output_gradient = ?)\n",
        "      grad = dlayer.backward(?, ?)\n",
        "      b = 0\n",
        "      epoch = epoch + 1\n",
        "      b += 1"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
