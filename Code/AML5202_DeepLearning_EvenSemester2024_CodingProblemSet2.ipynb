{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5dEgRpy3952M"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\adity\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## Load libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "from keras.datasets import mnist\n",
        "plt.style.use('dark_background')\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "G9W_1_v_6yq7"
      },
      "outputs": [],
      "source": [
        "np.set_printoptions(precision=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4T7eUtw7Mh0z"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Q1e2N5S8MlCU"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'2.15.0'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16BpVeIWIOks"
      },
      "source": [
        "---\n",
        "\n",
        "Load MNIST Data\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "E5kaKFKSIQgu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MNIST set\n",
            "---------------------\n",
            "Number of training samples = 60000\n",
            "Number of features = 784\n",
            "Number of output labels = 10\n"
          ]
        }
      ],
      "source": [
        "## Load MNIST data\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = X_train.transpose(1, 2, 0)\n",
        "X_test = X_test.transpose(1, 2, 0)\n",
        "X_train = X_train.reshape(X_train.shape[0]*X_train.shape[1], X_train.shape[2])\n",
        "X_test = X_test.reshape(X_test.shape[0]*X_test.shape[1], X_test.shape[2])\n",
        "\n",
        "num_labels = len(np.unique(y_train))\n",
        "num_features = X_train.shape[0]\n",
        "num_samples = X_train.shape[1]\n",
        "\n",
        "# One-hot encode class labels\n",
        "Y_train = tf.keras.utils.to_categorical(y_train).T\n",
        "Y_test = tf.keras.utils.to_categorical(y_test).T\n",
        "\n",
        "\n",
        "# Normalize the samples (images)\n",
        "xmax = np.amax(X_train)\n",
        "xmin = np.amin(X_train)\n",
        "X_train = (X_train - xmin) / (xmax - xmin) # all train features turn into a number between 0 and 1\n",
        "X_test = (X_test - xmin)/(xmax - xmin)\n",
        "\n",
        "print('MNIST set')\n",
        "print('---------------------')\n",
        "print('Number of training samples = %d'%(num_samples))\n",
        "print('Number of features = %d'%(num_features))\n",
        "print('Number of output labels = %d'%(num_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrXipxwrJ0_8"
      },
      "source": [
        "---\n",
        "\n",
        "A generic layer class with forward and backward methods\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "N4pKUhCyMrWm"
      },
      "outputs": [],
      "source": [
        "class Layer:\n",
        "  def __init__(self):\n",
        "    self.input = None\n",
        "    self.output = None\n",
        "\n",
        "  def forward(self, input):\n",
        "    pass\n",
        "\n",
        "  def backward(self, output_gradient, learning_rate):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdLfiQSlOSUU"
      },
      "source": [
        "---\n",
        "\n",
        "The softmax classifier steps for a batch of comprising $b$ samples represented as the $725\\times b$-matrix (724 pixel values plus the bias feature absorbed as its last row) $$\\mathbf{X} = \\begin{bmatrix}\\mathbf{x}^{(0)},\\mathbf{x}^{(1)},\\ldots,\\mathbf{x}^{(b-1)}\\end{bmatrix}$$ with one-hot encoded true labels represented as the $10\\times b$-matrix (10 possible categories) $$\\mathbf{Y}=\\begin{bmatrix}\\mathbf{y}^{(0)}&\\ldots&\\mathbf{y}^{(b-1)}\\end{bmatrix}$$ using a randomly initialized $10\\times725$-weights matrix $\\mathbf{W}$:\n",
        "\n",
        "1. Calculate $10\\times b$-raw scores matrix : $$\\begin{align*}\\begin{bmatrix}\\mathbf{z}^{(0)}&\\ldots&\\mathbf{z}^{(b-1)}\\ldots\\end{bmatrix} &= \\mathbf{W}\\begin{bmatrix}\\mathbf{z}^{(0)}&\\ldots&\\mathbf{z}^{(b-1)}\\ldots\\end{bmatrix}\\\\&=\\begin{bmatrix}\\mathbf{W}\\mathbf{z}^{(0)}&\\ldots&\\mathbf{W}\\mathbf{z}^{(b-1)}\\end{bmatrix}\\\\\\Rightarrow \\mathbf{Z} &= \\mathbf{WX}.\\end{align*}$$\n",
        "2. Calculate $10\\times b$-softmax predicted probabilities matrix: $$\\begin{align*}\\begin{bmatrix}\\mathbf{a}^{(0)}&\\ldots&\\mathbf{a}^{(b-1)}\\end{bmatrix} &= \\begin{bmatrix}\\text{softmax}\\left(\\mathbf{z}^{(0)}\\right)&\\ldots&\\text{softmax}\\left(\\mathbf{z}^{(b-1)}\\right)\\end{bmatrix}\\\\\\Rightarrow\\mathbf{A} &= \\text{softmax}(\\mathbf{Z}).\\end{align*}$$\n",
        "3. Predicted probability matrix get a new name: $\\hat{\\mathbf{Y}} = \\mathbf{A}.$\n",
        "4. The crossentropy (CCE) loss for the $i$th sample is $$L_i = \\sum_{k=0}^9-y^{(i)}\\log\\left(\\hat{y}^{(i)}_k\\right) = -{\\mathbf{y}^{(i)}}^\\mathrm{T}\\log\\left(\\mathbf{y}^{(i)}\\right)$$ which leads to the average crossentropy (CCE) batch loss for the batch as:\n",
        "$$\\begin{align*}L &=\\frac{1}{b}\\left[L_0+\\cdots+L_{b-1}\\right]\\\\&=\\frac{1}{b}\\left[-{\\mathbf{y}^{(0)}}^{\\mathrm{T}}\\log\\left(\\hat{\\mathbf{y}}^{(0)}\\right)+\\cdots+-{\\mathbf{y}^{(b-1)}}^{\\mathrm{T}}\\log\\left(\\hat{\\mathbf{y}}^{(b-1)}\\right)\\right].\\end{align*}.$$\n",
        "5. The computational graph for the samples in the batch are presented below:\n",
        "\n",
        "$\\hspace{1.5in}\\begin{align*}L_0\\\\{\\color{yellow}\\downarrow}\\\\ \\hat{\\mathbf{y}}^{(0)} &= \\mathbf{a}^{(0)}\\\\{\\color{yellow}\\downarrow}\\\\\\mathbf{z}^{(0)}\\\\{\\color{yellow}\\downarrow}\\\\\\mathbf{W}\\end{align*}$$\\qquad\\cdots\\qquad$$\\begin{align*} L_{b-1}\\\\{\\color{yellow}\\downarrow}\\\\ \\hat{\\mathbf{y}}^{(b-1)} &= \\mathbf{a}^{(b-1)}\\\\{\\color{yellow}\\downarrow}\\\\\\mathbf{z}^{(b-1)}\\\\{\\color{yellow}\\downarrow}\\\\\\mathbf{W}\\end{align*}$\n",
        "6. Calculate the gradient of the average batch loss w.r.t. weights as: $$\\begin{align*}\\Rightarrow \\nabla_\\mathbf{W}(L) &= \\frac{1}{b}\\left(\\underbrace{\\left[\\nabla_\\mathbf{W}\\left(\\mathbf{z}^{(0)}\\right) \\times\\nabla_{\\mathbf{z}^{(0)}}\\left(\\hat{\\mathbf{y}}^{(0)}\\right)\\times\\nabla_{\\hat{\\mathbf{y}}^{(0)}}(L_0)\\right]}_{\\text{sample}\\,0}+\\cdots+\\underbrace{\\left[\\nabla_\\mathbf{W}\\left(\\mathbf{z}^{(b-1)}\\right) \\times\\nabla_{\\mathbf{z}^{(b-1)}}\\left(\\hat{\\mathbf{y}}^{(b-1)}\\right)\\times\\nabla_{\\hat{\\mathbf{y}}^{(b-1)}}(L_{b-1})\\right]}_{\\text{sample}\\,b-1}\\right)\\\\&=\\frac{1}{b}\\left(\\underbrace{\\left[\\nabla_\\mathbf{W}\\left(\\mathbf{z}^{(0)}\\right) \\times\\nabla_{\\mathbf{z}^{(0)}}\\left({\\mathbf{a}}^{(0)}\\right)\\times\\nabla_{\\hat{\\mathbf{y}}^{(0)}}(L_0)\\right]}_{\\text{sample}\\,0}+\\cdots+\\underbrace{\\left[\\nabla_\\mathbf{W}\\left(\\mathbf{z}^{(b-1)}\\right) \\times\\nabla_{\\mathbf{z}^{(b-1)}}\\left(\\hat{\\mathbf{y}}^{(b-1)}\\right)\\times\\nabla_{\\hat{\\mathbf{y}}^{(b-1)}}(L_{b-1})\\right]}_{\\text{sample}\\,b-1}\\right).\\end{align*}$$\n",
        "10. The full gradient can be written as $\\nabla_\\mathbf{W}(L)=$\n",
        "\n",
        "![](https://onedrive.live.com/embed?resid=37720F927B6DDC34%21103292&authkey=%21AMoosVj6GqUSvpc&width=660)\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9YGwzbz72CZ"
      },
      "source": [
        "---\n",
        "\n",
        "CCE loss and its gradient for the batch samples:\n",
        "\n",
        "$$\\begin{align*}L &=\\frac{1}{b}\\left[L_0+\\cdots+L_{b-1}\\right]\\\\&=\\frac{1}{b}\\left[-{\\mathbf{y}^{(0)}}^{\\mathrm{T}}\\log\\left(\\hat{\\mathbf{y}}^{(0)}\\right)+\\cdots+-{\\mathbf{y}^{(b-1)}}^{\\mathrm{T}}\\log\\left(\\hat{\\mathbf{y}}^{(b-1)}\\right)\\right].\\end{align*}.$$\n",
        "\n",
        "$$\\begin{align*}\\begin{bmatrix}\\nabla_{\\hat{\\mathbf{y}}^{(0)}}(L_0)&\\ldots&\\nabla_{\\hat{\\mathbf{y}}^{(b-1)}}(L_{b-1})\\end{bmatrix}=\\begin{bmatrix}-y_0^{(0)}/\\hat{y}_0^{(0)}&\\cdots&-y_0^{(0)}/\\hat{y}_0^{(b-1)}\\\\-y_1^{(0)}/\\hat{y}_1^{(0)}&\\ldots&-y_1^{(b-1)}/\\hat{y}_1^{(b-1)}\\\\-y_2^{(0)}/\\hat{y}_2^{(0)}&\\cdots&-y_2^{(b-1)}/\\hat{y}_2^{(b-1)}\\\\\\vdots\\\\-y_9^{(0)}/\\hat{y}_9^{(0)}&\\cdots&-y_9^{(b-1)}/\\hat{y}_9^{(b-1)}\\end{bmatrix}\\end{align*}$$\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdXSGW2s7zKd"
      },
      "outputs": [],
      "source": [
        "## Define the loss function and its gradient\n",
        "def cce(Y, Yhat):\n",
        "  return(np.mean(np.?(?*?, axis = ?)))\n",
        "\n",
        "def cce_gradient(Y, Yhat):\n",
        "  return(?/?)\n",
        "\n",
        "# TensorFlow in-built function for categorical crossentropy loss\n",
        "#cce = tf.keras.losses.CategoricalCrossentropy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smgXLg9p65HV"
      },
      "source": [
        "---\n",
        "\n",
        "Softmax activation layer class:\n",
        "\n",
        "**Forward**:\n",
        "$$\\begin{align*}\\begin{bmatrix}\\mathbf{a}^{(0)}&\\ldots&\\mathbf{a}^{(b-1)}\\end{bmatrix} &= \\begin{bmatrix}\\text{softmax}\\left(\\mathbf{z}^{(0)}\\right)&\\ldots&\\text{softmax}\\left(\\mathbf{z}^{(b-1)}\\right)\\end{bmatrix}\\\\\\Rightarrow\\mathbf{A} &= \\text{softmax}(\\mathbf{Z}).\\end{align*}$$\n",
        "\n",
        "**Backward**:\n",
        "$$\\begin{align*}\\begin{bmatrix}\\nabla_{\\mathbf{z}^{(0)}}(L_0)&\\ldots&\\nabla_{\\mathbf{z}^{(b-1)}}(L_{b-1})\\end{bmatrix} &= \\begin{bmatrix}\\nabla_{\\mathbf{z}^{(0)}}\\left({\\mathbf{a}}^{(0)}\\right)\\times\\nabla_{\\mathbf{a}^{(0)}}(L_0)&\\cdots&\\nabla_{\\mathbf{z}^{(b-1)}}\\left({\\mathbf{a}}^{(b-1)}\\right)\\times\\nabla_{\\mathbf{a}^{(b-1)}}(L_{b-1})\\end{bmatrix}\\end{align*}$$\n",
        "\n",
        "![](https://onedrive.live.com/embed?resid=37720F927B6DDC34%21103299&authkey=%21AIPPR63BJ3UybA8&width=928&height=99)\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4x1Xn3AbJlNy"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (1946647092.py, line 10)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;36m  Cell \u001b[1;32mIn[10], line 10\u001b[1;36m\u001b[0m\n\u001b[1;33m    softmax_gradient[:, ?] = np.dot((np.identity(self.output.shape[0])-self.?[:, ?].T) * self.output[?, ?], ?[:, b])\u001b[0m\n\u001b[1;37m                        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "## Softmax activation layer class\n",
        "class Softmax(Layer):\n",
        "  def forward(self, input):\n",
        "    self.output = tf.nn.softmax(self.input, axis = 0).numpy()\n",
        "\n",
        "  def backward(self, output_gradient, learning_rate = None):\n",
        "    ## Following is the inefficient way of calculating the backward gradient\n",
        "    softmax_gradient = np.empty((self.input.shape[0], output_gradient.shape[1]), dtype = np.float64)\n",
        "    for b in range(softmax_gradient.shape[1]):\n",
        "      softmax_gradient[:, ?] = np.dot((np.identity(self.output.shape[0])-self.?[:, ?].T) * self.output[?, ?], ?[:, b])\n",
        "    return(softmax_gradient)\n",
        "    ## Following is the efficient of calculating the backward gradient\n",
        "    #T = (np.transpose(np.identity(self.output.shape[0]) - np.atleast_2d(self.output).T[:, np.newaxis, :], (1, 2, 0)) * np.atleast_2d(self.output))\n",
        "    #return(np.einsum('ijk, ik -> jk', T, output_gradient))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkPFfd1U68dj"
      },
      "source": [
        "---\n",
        "\n",
        "Dense layer class:\n",
        "\n",
        "**Forward**:\n",
        "$$$$\\begin{align*}\\begin{bmatrix}\\mathbf{z}^{(0)}&\\ldots&\\mathbf{z}^{(b-1)}\\ldots\\end{bmatrix} &= \\mathbf{W}\\begin{bmatrix}\\mathbf{z}^{(0)}&\\ldots&\\mathbf{z}^{(b-1)}\\ldots\\end{bmatrix}\\\\&=\\begin{bmatrix}\\mathbf{W}\\mathbf{z}^{(0)}&\\ldots&\\mathbf{W}\\mathbf{z}^{(b-1)}\\end{bmatrix}\\\\\\Rightarrow \\mathbf{Z} &= \\mathbf{WX}.\\end{align*}$$$$\n",
        "\n",
        "**Backward**:\n",
        "$$\\begin{align*}\\nabla_\\mathbf{W}(L)&=\\frac{1}{b}\\left[\\nabla_{\\mathbf{W}}(\\mathbf{z}^{(0)})\\times\\nabla_{\\mathbf{z^{(0)}}}(L) +\\cdots+ \\nabla_{\\mathbf{W}}(\\mathbf{z}^{(b-1)})\\times\\nabla_{\\mathbf{z^{(b-1)}}}(L)\\right]\\\\&=\\frac{1}{b}\\left[\\nabla_{\\mathbf{z^{(0)}}}(L){\\mathbf{x}^{(0)}}^\\mathrm{T}+\\cdots+\\nabla_{\\mathbf{z^{(b-1)}}}(L) {\\mathbf{x}^{(b-1)}}^\\mathrm{T}\\right].\\end{align*}$$\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ctXhZYCTmHK"
      },
      "outputs": [],
      "source": [
        "## Dense layer class\n",
        "class Dense(Layer):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.weights = 0.01*np.random.randn(output_size, input_size+1) # bias trick\n",
        "        self.weights[:, ?] = 0.01 # set all bias values to the same nonzero constant\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.input = np.vstack([?, np.ones((1, input.shape[?]))]) # bias trick\n",
        "        self.output= np.dot(?, ?)\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        ## Following is the inefficient way of calculating the backward gradient\n",
        "        dense_gradient = np.zeros((self.output.shape[?], self.input.shape[?]), dtype = np.float64)\n",
        "        for b in range(output_gradient.shape[1]):\n",
        "          dense_gradient += np.dot(output_gradient[?, b].reshape(-1, 1), self.input[:, ?].reshape(-1, 1).T)\n",
        "        dense_gradient = (1/output_gradient.shape[1])*dense_gradient\n",
        "        ## Following is the efficient way of calculating the backward gradient\n",
        "        #dense_gradient = (1/output_gradient.shape[1])*np.dot(np.atleast_2d(output_gradient), np.atleast_2d(self.input).T)\n",
        "        self.weights = self.weights + learning_rate * (-dense_gradient)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2W1howeOJegI"
      },
      "source": [
        "---\n",
        "\n",
        "Function to generate sample indices for batch processing according to batch size\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MHyjEf22IRpc"
      },
      "outputs": [],
      "source": [
        "## Function to generate sample indices for batch processing according to batch size\n",
        "def generate_batch_indices(num_samples, batch_size):\n",
        "  # Reorder sample indices\n",
        "  reordered_sample_indices = np.random.choice(num_samples, num_samples, replace = False)\n",
        "  # Generate batch indices for batch processing\n",
        "  batch_indices = np.split(reordered_sample_indices, np.arange(batch_size, len(reordered_sample_indices), batch_size))\n",
        "  return(batch_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKFmCaFsJhkR"
      },
      "source": [
        "---\n",
        "\n",
        "Example generation of batch indices\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9QwikN0IYSp"
      },
      "outputs": [],
      "source": [
        "## Example generation of batch indices\n",
        "batch_size = 100\n",
        "batch_indices = generate_batch_indices(num_samples, batch_size)\n",
        "print(batch_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fI_Gms9fJqbs"
      },
      "source": [
        "---\n",
        "\n",
        "Train the 0-layer neural network using batch training with batch size = 16\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGIzrN-rPuI4"
      },
      "outputs": [],
      "source": [
        "## Train the 0-layer neural network using batch training with batch size = 16\n",
        "learning_rate = ? # learning rate\n",
        "batch_size = ? # batch size\n",
        "nepochs = ? # number of epochs\n",
        "loss_epoch = np.empty(nepochs, dtype = np.float32) # create empty array to store losses over each epoch\n",
        "\n",
        "# Neural network architecture\n",
        "dlayer = Dense(?, ?) # define dense layer\n",
        "softmax = Softmax() # define softmax activation layer\n",
        "\n",
        "# Steps: run over each sample in the batch, calculate loss, gradient of loss,\n",
        "# and update weights.\n",
        "\n",
        "epoch = 0\n",
        "while epoch < nepochs:\n",
        "  batch_indices = generate_batch_indices(num_samples, batch_size)\n",
        "  loss = 0\n",
        "  for b in range(len(batch_indices)):\n",
        "    dlayer.forward(?) # forward prop\n",
        "    softmax.forward(?) # Softmax activate\n",
        "    loss += cce(?, ?) # calculate loss\n",
        "    # Backward prop starts here\n",
        "    grad = cce_gradient(?, ?)\n",
        "    grad = softmax.backward(?)\n",
        "    grad = dlayer.backward(?, ?)\n",
        "  loss_epoch[epoch] = loss/len(batch_indices)\n",
        "  print('Epoch %d: loss = %f'%(epoch+1, loss_epoch[epoch]))\n",
        "  epoch = epoch + 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Iv3k23SlCqGf"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'loss_epoch' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## Plot training loss as a function of epoch:\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(loss_epoch)\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss value\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'loss_epoch' is not defined"
          ]
        }
      ],
      "source": [
        "## Plot training loss as a function of epoch:\n",
        "plt.plot(loss_epoch)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss value')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "d7AEbmpcKcPY"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'dlayer' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## Accuracy on test set\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m dlayer\u001b[38;5;241m.\u001b[39mforward(X_test)\n\u001b[0;32m      3\u001b[0m softmax\u001b[38;5;241m.\u001b[39mforward(dlayer\u001b[38;5;241m.\u001b[39moutput)\n\u001b[0;32m      4\u001b[0m ypred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(softmax\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mT, axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'dlayer' is not defined"
          ]
        }
      ],
      "source": [
        "## Accuracy on test set\n",
        "dlayer.forward(X_test)\n",
        "softmax.forward(dlayer.output)\n",
        "ypred = np.argmax(softmax.output.T, axis = 1)\n",
        "print(ypred)\n",
        "ytrue = np.argmax(Y_test.T, axis = 1)\n",
        "print(ytrue)\n",
        "np.mean(ytrue == ypred)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
