{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"5dEgRpy3952M"},"outputs":[],"source":["## Load libraries\n","import pandas as pd\n","import numpy as np\n","import sys\n","import matplotlib.pyplot as plt\n","import matplotlib.cm as cm\n","from keras.datasets import mnist\n","plt.style.use('dark_background')\n","%matplotlib inline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G9W_1_v_6yq7"},"outputs":[],"source":["np.set_printoptions(precision=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4T7eUtw7Mh0z"},"outputs":[],"source":["import tensorflow as tf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q1e2N5S8MlCU"},"outputs":[],"source":["tf.__version__"]},{"cell_type":"markdown","metadata":{"id":"16BpVeIWIOks"},"source":["---\n","\n","Load MNIST Data\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E5kaKFKSIQgu"},"outputs":[],"source":["## Load MNIST data\n","(X_train, y_train), (X_test, y_test) = mnist.load_data()\n","X_train = X_train.transpose(1, 2, 0)\n","X_test = X_test.transpose(1, 2, 0)\n","X_train = X_train.reshape(X_train.shape[0]*X_train.shape[1], X_train.shape[2])\n","X_test = X_test.reshape(X_test.shape[0]*X_test.shape[1], X_test.shape[2])\n","\n","num_labels = len(np.unique(y_train))\n","num_features = X_train.shape[0]\n","num_samples = X_train.shape[1]\n","\n","# One-hot encode class labels\n","Y_train = tf.keras.utils.to_categorical(y_train).T\n","Y_test = tf.keras.utils.to_categorical(y_test).T\n","\n","\n","# Normalize the samples (images)\n","xmax = np.amax(X_train)\n","xmin = np.amin(X_train)\n","X_train = (X_train - xmin) / (xmax - xmin) # all train features turn into a number between 0 and 1\n","X_test = (X_test - xmin)/(xmax - xmin)\n","\n","print('MNIST set')\n","print('---------------------')\n","print('Number of training samples = %d'%(num_samples))\n","print('Number of features = %d'%(num_features))\n","print('Number of output labels = %d'%(num_labels))"]},{"cell_type":"markdown","metadata":{"id":"IrXipxwrJ0_8"},"source":["---\n","\n","A generic layer class with forward and backward methods\n","\n","----"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N4pKUhCyMrWm"},"outputs":[],"source":["class Layer:\n","  def __init__(self):\n","    self.input = None\n","    self.output = None\n","\n","  def forward(self, input):\n","    pass\n","\n","  def backward(self, output_gradient, learning_rate):\n","    pass"]},{"cell_type":"markdown","source":["---\n","\n","CCE loss and its gradient\n","\n","---"],"metadata":{"id":"UMt81Faf9-bf"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"hdXSGW2s7zKd"},"outputs":[],"source":["## Define the loss function and its gradient\n","def cce(Y, Yhat):\n","  return(np.mean(np.sum(-Y*np.log(Yhat), axis = 0)))\n","  #TensorFlow in-built function for categorical crossentropy loss\n","  #cce = tf.keras.losses.CategoricalCrossentropy()\n","  #return(cce(Y, Yhat).numpy())\n","\n","def cce_gradient(Y, Yhat):\n","  return(-Y/Yhat)"]},{"cell_type":"markdown","source":["---\n","\n","Generic activation layer class\n","\n","---"],"metadata":{"id":"jmcNJTjS-BaW"}},{"cell_type":"code","source":["class Activation(Layer):\n","    def __init__(self, activation, activation_gradient):\n","        self.activation = activation\n","        self.activation_gradient = activation_gradient\n","\n","    def forward(self, input):\n","        self.input = input\n","        self.output = self.activation(self.input)\n","        return(self.output)\n","\n","    def backward(self, output_gradient, learning_rate = None):\n","        return(output_gradient[:-1, :] * self.activation_gradient(self.input))"],"metadata":{"id":"C21FcWIEwGCN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Specific activation layer classes\n","\n","---"],"metadata":{"id":"JheGWSoKxYWu"}},{"cell_type":"code","source":["class Sigmoid(Activation):\n","    def __init__(self):\n","        def sigmoid(z):\n","            return 1 / (1 + np.exp(-z))\n","\n","        def sigmoid_gradient(z):\n","            a = sigmoid(z)\n","            return a * (1 - a)\n","\n","        super().__init__(sigmoid, sigmoid_gradient)\n","\n","class Tanh(Activation):\n","    def __init__(self):\n","        def tanh(z):\n","            return np.tanh(z)\n","\n","        def tanh_gradient(z):\n","            a = np.tanh(z)\n","            return 1 - a**2\n","\n","        super().__init__(tanh, tanh_gradient)\n","\n","class ReLU(Activation):\n","    def __init__(self):\n","        def relu(z):\n","            return z * (z > 0)\n","\n","        def relu_gradient(z):\n","            return 1. * (z > 0)\n","\n","        super().__init__(relu, relu_gradient)"],"metadata":{"id":"PQ5ybz_Yxbef"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Softmax activation layer class\n","\n","---"],"metadata":{"id":"LGdr2m2R-It8"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"4x1Xn3AbJlNy"},"outputs":[],"source":["## Softmax activation layer class\n","class Softmax(Layer):\n","  def forward(self, input):\n","    self.output = tf.nn.softmax(input, axis = 0).numpy()\n","\n","  def backward(self, output_gradient, learning_rate = None):\n","    ## Following is the inefficient way of calculating the backward gradient\n","    softmax_gradient = np.empty((self.output.shape[0], output_gradient.shape[1]), dtype = np.float64)\n","    #for b in range(softmax_gradient.shape[1]):\n","    #  softmax_gradient[:, b] = np.dot((np.identity(self.output.shape[0])-np.atleast_2d(self.output[:, b])) * np.atleast_2d(self.output[:, b]).T, output_gradient[:, b])\n","    #return(softmax_gradient)\n","    ## Following is the efficient of calculating the backward gradient\n","    T = np.transpose(np.identity(self.output.shape[0]) - np.atleast_2d(self.output).T[:, np.newaxis, :], (2, 1, 0)) * np.atleast_2d(self.output)\n","    return(np.einsum('jik, ik -> jk', T, output_gradient))"]},{"cell_type":"markdown","source":["---\n","\n","Dense layer class\n","\n","---"],"metadata":{"id":"UKnqi7rf-MBn"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"8ctXhZYCTmHK"},"outputs":[],"source":["## Dense layer class\n","class Dense(Layer):\n","    def __init__(self, input_size, output_size, reg_strength):\n","        self.weights = 0.01*np.random.randn(output_size, input_size+1) # bias trick\n","        self.weights[:, -1] = 0.01 # set all bias values to the same nonzero constant\n","        self.reg_strength = reg_strength\n","        self.reg_loss = None\n","\n","    def forward(self, input):\n","        self.input = np.vstack([input, np.ones((1, input.shape[1]))]) # bias trick\n","        self.output= np.dot(self.weights, self.input)\n","        # Calculate regularization loss\n","        self.reg_loss = self.reg_strength * np.sum(self.weights[:, :-1] * self.weights[:, :-1])\n","\n","    def backward(self, output_gradient, learning_rate):\n","        ## Following is the inefficient way of calculating the backward gradient\n","        #weights_gradient = np.zeros((self.output.shape[0], self.input.shape[0]), dtype = np.float64)\n","        #for b in range(output_gradient.shape[1]):\n","        #  weights_gradient += np.dot(output_gradient[:, b].reshape(-1, 1), self.input[:, b].reshape(-1, 1).T)\n","        #weights_gradient = (1/output_gradient.shape[1])*weights_gradient\n","\n","        ## Following is the efficient way of calculating the weights gradient w.r.t. data\n","        weights_gradient = (1/output_gradient.shape[1])*np.dot(np.atleast_2d(output_gradient), np.atleast_2d(self.input).T)\n","        # Add the regularization gradient here\n","        weights_gradient += 2 * self.reg_strength * np.hstack([self.weights[:, :-1], np.zeros((self.weights.shape[0], 1))])\n","\n","\n","        input_gradient = np.dot(self.weights.T, output_gradient)\n","        self.weights = self.weights + learning_rate * (-weights_gradient)\n","\n","        return(input_gradient)"]},{"cell_type":"markdown","metadata":{"id":"2W1howeOJegI"},"source":["---\n","\n","Function to generate sample indices for batch processing according to batch size\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MHyjEf22IRpc"},"outputs":[],"source":["## Function to generate sample indices for batch processing according to batch size\n","def generate_batch_indices(num_samples, batch_size):\n","  # Reorder sample indices\n","  reordered_sample_indices = np.random.choice(num_samples, num_samples, replace = False)\n","  # Generate batch indices for batch processing\n","  batch_indices = np.split(reordered_sample_indices, np.arange(batch_size, len(reordered_sample_indices), batch_size))\n","  return(batch_indices)"]},{"cell_type":"markdown","metadata":{"id":"fI_Gms9fJqbs"},"source":["---\n","\n","Train the 1-hidden layer neural network (128 nodes) using batch training with batch size = 100\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LGIzrN-rPuI4"},"outputs":[],"source":["## Train the 2-hidden layer neural network (128 nodes)\n","## using batch training with batch size = 100\n","learning_rate = 2 # learning rate\n","batch_size = 100 # batch size\n","nepochs = 100 # number of epochs\n","reg_strength = 0 # regularization strength\n","# Create empty array to store training losses over each epoch\n","loss_train_epoch = np.empty(nepochs, dtype = np.float64)\n","# Create empty array to store test losses over each epoch\n","loss_test_epoch = np.empty(nepochs, dtype = np.float64)\n","\n","\n","# Neural network architecture\n","\n","dlayer1 = Dense(num_features, 128, reg_strength) # define dense layer 1\n","alayer1 = ReLU() # ReLU activation layer 1\n","dlayer2 = Dense(128, num_labels, reg_strength) # define dense layer 2\n","softmax = Softmax() # define softmax activation layer\n","\n","# Steps: run over each sample in the batch, calculate loss, gradient of loss,\n","# and update weights.\n","\n","epoch = 0\n","while epoch < nepochs:\n","  batch_indices = generate_batch_indices(num_samples, batch_size)\n","  loss = 0\n","  for b in range(len(batch_indices)):\n","    # Forward propagation for training data\n","    dlayer1.forward(X_train[:, batch_indices[b]]) # forward prop dense layer 1 with batch feature added\n","    alayer1.forward(dlayer1.output) # forward prop activation layer 1\n","    dlayer2.forward(alayer1.output) # forward prop dense layer 2\n","    softmax.forward(dlayer2.output) # Softmax activate\n","    # Calculate training data loss\n","    loss += cce(Y_train[:, batch_indices[b]], softmax.output)\n","    # Add the regularization losses\n","    loss += dlayer1.reg_loss + dlayer2.reg_loss\n","\n","    # Backward prop starts here\n","    grad = cce_gradient(Y_train[:, batch_indices[b]], softmax.output)\n","    grad = softmax.backward(grad)\n","    grad = dlayer2.backward(grad, learning_rate)\n","    grad = alayer1.backward(grad)\n","    grad = dlayer1.backward(grad, learning_rate)\n","  # Calculate the average training loss for the current epoch\n","  loss_train_epoch[epoch] = loss/len(batch_indices)\n","\n","  # Forward propagation for test data\n","  dlayer1.forward(X_test)\n","  alayer1.forward(dlayer1.output)\n","  dlayer2.forward(alayer1.output)\n","  softmax.forward(dlayer2.output)\n","\n","  # Calculate test data loss plus regularization loss\n","  loss_test_epoch[epoch] =  cce(Y_test, softmax.output) + dlayer1.reg_loss + dlayer2.reg_loss\n","\n","  print('Epoch %d: train loss = %f, test loss = %f'%(epoch+1, loss_train_epoch[epoch], loss_test_epoch[epoch]))\n","  epoch = epoch + 1"]},{"cell_type":"markdown","source":["---\n","\n","Plot training loss vs. epoch\n","\n","---"],"metadata":{"id":"EhXEFASk-Tkv"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Iv3k23SlCqGf"},"outputs":[],"source":["# Plot train and test loss as a function of epoch:\n","fig, ax = plt.subplots(1, 1, figsize = (4, 4))\n","fig.tight_layout(pad = 4.0)\n","ax.plot(loss_train_epoch, 'b', label = 'Train')\n","ax.plot(loss_test_epoch, 'r', label = 'Test')\n","ax.set_xlabel('Epoch', fontsize = 12)\n","ax.set_ylabel('Loss value', fontsize = 12)\n","ax.legend()\n","ax.set_title('Loss vs. Epoch for reg. strength 0.01', fontsize = 14)"]},{"cell_type":"markdown","source":["---\n","\n","Test performance on test data\n","\n","---"],"metadata":{"id":"TaLoOOWK-WBj"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"d7AEbmpcKcPY"},"outputs":[],"source":["dlayer1.forward(X_test)\n","alayer1.forward(dlayer1.output)\n","dlayer2.forward(alayer1.output)\n","softmax.forward(dlayer2.output)\n","ypred = np.argmax(softmax.output.T, axis = 1)\n","print(ypred)\n","ytrue = np.argmax(Y_test.T, axis = 1)\n","print(ytrue)\n","np.mean(ytrue == ypred)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}