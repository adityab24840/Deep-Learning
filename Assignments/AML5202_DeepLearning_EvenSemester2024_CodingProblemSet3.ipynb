{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.12.1'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from keras.datasets import mnist\n",
    "plt.style.use('dark_background')\n",
    "%matplotlib inline\n",
    "np.set_printoptions(precision=2)\n",
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load MNIST data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.transpose(1, 2, 0)\n",
    "X_test = X_test.transpose(1, 2, 0)\n",
    "X_train = X_train.reshape(X_train.shape[0]*X_train.shape[1], X_train.shape[2])\n",
    "X_test = X_test.reshape(X_test.shape[0]*X_test.shape[1], X_test.shape[2])\n",
    "num_labels = len(np.unique(y_train))\n",
    "num_features = X_train.shape[0]\n",
    "num_samples = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode class labels\n",
    "Y_train = tf.keras.utils.to_categorical(y_train).T\n",
    "Y_test = tf.keras.utils.to_categorical(y_test).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the samples (images)\n",
    "xmax = np.amax(X_train)\n",
    "xmin = np.amin(X_train)\n",
    "X_train = (X_train - xmin) / (xmax - xmin) # all train features turn into a number between 0 and 1\n",
    "X_test = (X_test - xmin)/(xmax - xmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST set\n",
      "---------------------\n",
      "Number of training samples = 60000\n",
      "Number of features = 784\n",
      "Number of output labels = 10\n"
     ]
    }
   ],
   "source": [
    "print('MNIST set')\n",
    "print('---------------------')\n",
    "print('Number of training samples = %d'%(num_samples))\n",
    "print('Number of features = %d'%(num_features))\n",
    "print('Number of output labels = %d'%(num_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A generic layer class with forward and backward methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "    \n",
    "    def forward(self, input):\n",
    "        pass\n",
    "    \n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CCE loss and its gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cce_gradient(Y, Yhat):\n",
    "    return (-Y/Yhat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generic activation layer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(Layer):\n",
    "    def __init__(self, activation, activation_gradient):\n",
    "        self.activation = activation\n",
    "        self.activation_gradient = activation_gradient\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        self.output = self.activation(self.input)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate=None):\n",
    "        return output_gradient * self.activation_gradient(self.input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specific activation layer classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Activation):\n",
    "    def __init__(self):\n",
    "        def sigmoid(z):\n",
    "            return 1 / (1 + np.exp(-z))\n",
    "        def sigmoid_gradient(z):\n",
    "            a = sigmoid(z)\n",
    "            return a * (1 - a)\n",
    "        super().__init__(sigmoid, sigmoid_gradient)\n",
    "\n",
    "class Tanh(Activation):\n",
    "    def __init__(self):\n",
    "        def tanh(z):\n",
    "            return np.tanh(z)\n",
    "        def tanh_gradient(z):\n",
    "            return 1 - np.tanh(z) ** 2\n",
    "        super().__init__(tanh, tanh_gradient)\n",
    "\n",
    "class ReLU(Activation):\n",
    "    def __init__(self):\n",
    "        def relu(z):\n",
    "            return np.maximum(0, z)\n",
    "        def relu_gradient(z):\n",
    "            return 1. * (z > 0)\n",
    "        super().__init__(relu, relu_gradient)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax activation layer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Softmax activation layer class\n",
    "class Softmax(Layer):\n",
    "    def forward(self, input):\n",
    "        self.output = tf.nn.softmax(input, axis=0).numpy()\n",
    "    \n",
    "    def backward(self, output_gradient):\n",
    "        softmax_gradient = np.empty((self.output.shape[0], output_gradient.shape[1]), dtype=np.float64)\n",
    "        for b in range(softmax_gradient.shape[1]):\n",
    "            softmax_gradient[:, b] = np.dot((np.identity(self.output.shape[0])) - np.atleast_2d(self.output[:, b]), np.atleast_2d(self.output[:, b]))\n",
    "        # Return gradient w.r.t. input for backward propagation\n",
    "        return softmax_gradient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dense layer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dense layer class\n",
    "class Dense(Layer):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = 0.01*np.random.randn(output_size, input_size+1) # bias trick\n",
    "        self.weights[:, -1] = 0.01 # set all bias values to the same nonzero constant\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.input = np.vstack([input, np.ones((1, input.shape[1]))]) # bias trick\n",
    "        self.output= np.dot(self.weights, self.input)\n",
    "    \n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        ## Following is the inefficient way of calculating the gradient w.r.t. weights\n",
    "        weights_gradient = np.zeros((self.output.shape[0], self.input.shape[0]), dtype = np.float64)\n",
    "        for b in range(output_gradient.shape[1]):\n",
    "            weights_gradient += np.dot(output_gradient[:, b].reshape(-1, 1), self.input[:, b].reshape(-1, 1).T)\n",
    "        weights_gradient = (1/output_gradient.shape[1])*weights_gradient\n",
    "        # Following is the efficient way of calculating the weightsgradient\n",
    "        weights_gradient = (1/output_gradient.shape[1])*np.dot(np.atleast_2d(output_gradient), np.atleast_2d(self.input).T)\n",
    "        # Gradient w.r.t. the input\n",
    "        input_gradient = np.dot(self.weights.T, output_gradient)\n",
    "        # Update weights using gradient descent step\n",
    "        self.weights = self.weights + learning_rate * (-weights_gradient)\n",
    "        # Return gradient w.r.t. input for backward propagation\n",
    "        return input_gradient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to generate sample indices for batch processing according to batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to generate sample indices for batch processing according to batch size\n",
    "def generate_batch_indices(num_samples, batch_size):\n",
    "    # Reorder sample indices\n",
    "    reordered_sample_indices = np.random.choice(num_samples, num_samples, replace = False)\n",
    "    # Generate batch indices for batch processing\n",
    "    batch_indices = np.split(reordered_sample_indices, np.arange(batch_size, len(reordered_sample_indices), batch_size))\n",
    "    return(batch_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the 1-hidden layer neural network (128 nodes) using batch training with batch size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (700805528.py, line 45)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[80], line 45\u001b[1;36m\u001b[0m\n\u001b[1;33m    softmax.forward(?) # Softmax activate\u001b[0m\n\u001b[1;37m                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## Train the 1-hidden layer neural network (128 nodes)\n",
    "## using batch training with batch size = 100\n",
    "learning_rate = 1e-3 # learning rate\n",
    "batch_size = 100 # batch size\n",
    "nepochs = 200 # number of epochs\n",
    "loss_epoch = np.empty(nepochs, dtype = np.float64) # create empty array to store losses over each epoch\n",
    "# Neural network architecture\n",
    "dlayer1 = Dense(num_features, 128) # define dense layer 1\n",
    "alayer1 = ReLU() # ReLU activation layer 1\n",
    "dlayer2 = Dense(128, num_labels) # define dense layer 2\n",
    "softmax = Softmax() # define softmax activation layer\n",
    "# Steps: run over each sample in the batch, calculate loss, gradient of loss,\n",
    "# and update weights.\n",
    "epoch = 0\n",
    "while epoch < nepochs:\n",
    "    batch_indices = generate_batch_indices(num_samples, batch_size)\n",
    "    loss = 0\n",
    "    for b in range(len(batch_indices)):\n",
    "        dlayer1.forward(X_train[:, batch_indices[b]]) # forward prop dense layer 1 with batch feature added\n",
    "        alayer1.forward(dlayer1.output) # forward prop activation layer 1\n",
    "        dlayer2.forward(alayer1.output) # forward prop dense layer 2\n",
    "        softmax.forward(dlayer2.output) # Softmax activate\n",
    "        loss += cce(Y_train[:, batch_indices[b]], softmax.output) # calculate loss\n",
    "        # Backward prop starts here\n",
    "        grad = cce_gradient(Y_train[:, batch_indices[b]], softmax.output)\n",
    "        grad = softmax.backward(grad)\n",
    "        grad = dlayer2.backward(grad, learning_rate)\n",
    "        grad = alayer1.backward(grad)\n",
    "        grad = dlayer1.backward(grad, learning_rate)\n",
    "    softmax.forward(softmax.output) # Softmax activate\n",
    "    loss += cce(Y_train[:, batch_indices[b]], softmax.output) # calculate loss\n",
    "    # Backward prop starts here\n",
    "    grad = cce_gradient(Y_train[:, batch_indices[b]], softmax.output)\n",
    "    grad = softmax.backward(grad)\n",
    "    grad = dlayer2.backward(grad, learning_rate)\n",
    "    grad = alayer1.backward(grad)\n",
    "    grad = dlayer1.backward(grad, learning_rate)\n",
    "    loss_epoch[epoch] = loss/len(batch_indices)\n",
    "    print('Epoch %d: loss = %f'%(epoch+1, loss_epoch[epoch]))\n",
    "    epoch = epoch + 1\n",
    "    if softmax.output is not None:\n",
    "        grad = cce_gradient(Y_train[:, batch_indices[b]], softmax.output.T)\n",
    "    else:\n",
    "        grad = None\n",
    "softmax.forward(?) # Softmax activate\n",
    "loss += cce(Y_train[:, batch_indices[b]], softmax.output) # calculate loss\n",
    "# Backward prop starts here\n",
    "grad = cce_gradient(Y_train[:, batch_indices[b]], softmax.output)\n",
    "grad = softmax.backward(?)\n",
    "grad = dlayer2.backward(? learning_rate)\n",
    "grad = alayer1.backward(?)\n",
    "grad = dlayer1.backward(?, learning_rate)\n",
    "loss_epoch[epoch] = loss/len(batch_indices)\n",
    "print('Epoch %d: loss = %f'%(epoch+1, loss_epoch[epoch]))\n",
    "epoch = epoch + 1\n",
    "if softmax.output is not None:\n",
    "    grad = cce_gradient(Y_train[:, batch_indices[b]], softmax.output.T)\n",
    "else:\n",
    "    grad = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training loss as a function of epoch:\n",
    "plt.plot(loss_epoch)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss value')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
