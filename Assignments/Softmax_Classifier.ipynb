{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "5dEgRpy3952M"
      },
      "outputs": [],
      "source": [
        "## Load libraries\n",
        "import numpy as np\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "plt.style.use('dark_background')\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "4T7eUtw7Mh0z"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Q1e2N5S8MlCU"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'2.15.0'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "B4EjOi-OM4Gp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[3 4 7 7 3]\n",
            " [7 5 9 5 6]\n",
            " [7 3 3 3 6]\n",
            " [7 4 4 4 3]]\n",
            "------\n",
            "[0 1 0 0 1]\n",
            "------\n",
            "[[1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]]\n"
          ]
        }
      ],
      "source": [
        "# Generate artificial data with 5 samples, 4 features per sample\n",
        "# and 3 output classes\n",
        "num_samples = 5 # number of samples\n",
        "num_features = 4 # number of features (a.k.a. dimensionality)\n",
        "num_labels = 3 # number of output labels\n",
        "# Data matrix (each column = single sample)\n",
        "X = np.random.choice(np.arange(3, 10), size = (num_features, num_samples), replace = True)\n",
        "# Class labels\n",
        "y = np.random.choice([0, 1, 2], size = num_samples, replace = True)\n",
        "print(X)\n",
        "print('------')\n",
        "print(y)\n",
        "print('------')\n",
        "# One-hot encode class labels\n",
        "y = tf.keras.utils.to_categorical(y)\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrXipxwrJ0_8"
      },
      "source": [
        "---\n",
        "\n",
        "A generic layer class with forward and backward methods\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "N4pKUhCyMrWm"
      },
      "outputs": [],
      "source": [
        "class Layer:\n",
        "  def __init__(self):\n",
        "    self.input = None\n",
        "    self.output = None\n",
        "\n",
        "  def forward(self, input):\n",
        "    pass\n",
        "\n",
        "  def backward(self, output_gradient, learning_rate):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdLfiQSlOSUU"
      },
      "source": [
        "---\n",
        "\n",
        "The softmax classifier steps for a generic sample $\\mathbf{x}$ with (one-hot encoded) true label $\\mathbf{y}$ (3 possible categories) using a randomly initialized weights matrix (with bias abosrbed as its last last column):\n",
        "\n",
        "1. Calculate raw scores vector for a generic sample $\\mathbf{x}$  (bias feature added): $$\\mathbf{z} = \\mathbf{Wx}.$$\n",
        "2. Calculate softmax probabilities (that is, softmax-activate the raw scores) $$\\mathbf{a} = \\text{softmax}(\\mathbf{z})\\Rightarrow\\begin{bmatrix}a_0\\\\a_1\\\\a_2\\end{bmatrix}= \\text{softmax}\\left(\\begin{bmatrix}z_0\\\\z_1\\\\z_2\\end{bmatrix}\\right)=\\begin{bmatrix}\\frac{e^{z_0}}{e^{z_0}+e^{z_1}+e^{z_2}}\\\\\\frac{e^{z_1}}{e^{z_0}+e^{z_1}+e^{z_2}}\\\\\\frac{e^{z_2}}{e^{z_0}+e^{z_1}+e^{z_2}}\\end{bmatrix}$$\n",
        "3. Softmax loss for this sample is (where output label $y$ is not yet one-hot encoded)\n",
        "$$\\begin{align*}L &=  -\\log([a]_y) \\\\&= -\\log\\left(\\left[\\text{softmax}(\\mathbf{z})\\right]_y\\right)\\\\ &= -\\log\\left(\\left[\\text{softmax}(\\mathbf{Wx})\\right]_y\\right).\\end{align*}$$\n",
        "4. Predicted probability vector that the sample belongs to each one of the output categories is given a new name $$\\hat{\\mathbf{y}} = \\mathbf{a}.$$\n",
        "5. One-hot encoding the output label $$\\underbrace{y\\rightarrow\\mathbf{y}}_{\\text{e.g.}\\,2\\,\\rightarrow\\begin{bmatrix}0\\\\0\\\\1\\end{bmatrix}}$$ results in the following representation for the softmax loss for the sample which is also referred to as the categorical crossentropy (CCE) loss:\n",
        "$$\\begin{align*}L &= L\\left(\\mathbf{y},\\hat{\\mathbf{y}}\\right)=\\sum_{k=0}^2-y_k\\log\\left(\\hat{y}_k\\right)\\end{align*}.$$\n",
        "6. Calculate the gradient of the loss for the sample w.r.t. weights by following the computation graph from top to bottom (that is, backward):\n",
        "$$\\begin{align*} L\\\\{\\color{yellow}\\downarrow}\\\\ \\hat{\\mathbf{y}} &= \\mathbf{a}\\\\{\\color{yellow}\\downarrow}\\\\\\mathbf{z}\\\\{\\color{yellow}\\downarrow}\\\\\\mathbf{W}\\end{align*}$$\n",
        "$$\\begin{align*}\\Rightarrow \\nabla_\\mathbf{W}(L) &= \\nabla_\\mathbf{W}(\\mathbf{z}) \\times\\nabla_\\mathbf{z}(\\mathbf{a})\\times\\nabla_\\mathbf{a}(L)\\\\&= \\underbrace{\\nabla_\\mathbf{W}(\\mathbf{z})}_\\text{first term} \\times\\underbrace{\\nabla_\\mathbf{z}(\\mathbf{a})}_\\text{second to last term}\\times\\underbrace{\\nabla_\\hat{\\mathbf{y}}(L)}_\\text{last term}.\\end{align*}$$\n",
        "7. Now focus on the last term $\\nabla_\\hat{\\mathbf{y}}(L)$:\n",
        "$$\\begin{align*}\\nabla_\\hat{\\mathbf{y}}(L) &=\\begin{bmatrix}\\nabla_{\\hat{y}_0}(L)\\\\\\nabla_{\\hat{y}_1}(L)\\\\\\nabla_{\\hat{y}_2}(L)\\end{bmatrix} = \\begin{bmatrix}-y_0/\\hat{y}_0\\\\-y_1/\\hat{y}_2\\\\-y_0/\\hat{y}_2.\\end{bmatrix}\\end{align*}$$\n",
        "8. Now focus on the second to last term $\\nabla_\\mathbf{z}(\\mathbf{a})$:\n",
        "$$\\begin{align*}\\nabla_\\mathbf{z}(\\mathbf{a}) &= \\nabla_\\mathbf{z}\\left(\\begin{bmatrix}a_0\\\\a_1\\\\a_2\\end{bmatrix}\\right)\\\\ &= \\begin{bmatrix}\\nabla_\\mathbf{z}(a_0)&\\nabla_\\mathbf{z}(a_1)&\\nabla_\\mathbf{z}(a_2)\\end{bmatrix} \\\\&= \\begin{bmatrix}\\nabla_{z_0}(a_0)&\\nabla_{z_0}(a_1)&\\nabla_{z_0}(a_2)\\\\\\nabla_{z_1}(a_0)&\\nabla_{z_1}(a_1)&\\nabla_{z_1}(a_2)\\\\\\nabla_{z_2}(a_0)&\\nabla_{z_2}(a_1)&\\nabla_{z_2}(a_2)\\end{bmatrix}\\\\&=\\begin{bmatrix}a_0(1-a_0)&-a_1a_0&-a_2a_0\\\\-a_0a_1&a_1(1-a_1)&-a_1a_1\\\\-a_0a_2&-a_1a_2&a_2(1-a_2)\\end{bmatrix}.\\end{align*}$$\n",
        "9. On Monday, we will focus on the first term to complete the gradient calculation using the computation graph.\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "4x1Xn3AbJlNy"
      },
      "outputs": [],
      "source": [
        "## Softmax activation class\n",
        "class Softmax(Layer):\n",
        "  def forward(self, input):\n",
        "    self.output = np.array(tf.nn.softmax(input))\n",
        "\n",
        "  def backward(self, output_gradient, learning_rate):\n",
        "    return(np.dot((np.identity(np.size(self.output))-self.output.T) * self.output, output_gradient))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "tEmuLjCeRFVI"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<keras.src.losses.CategoricalCrossentropy at 0x191e8dfa910>"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Define the loss function and its gradient\n",
        "def cce(y, yhat):\n",
        "  return(-np.sum(y*np.log(yhat)))\n",
        "\n",
        "def cce_gradient(y, yhat):\n",
        "  return(-y/yhat)\n",
        "\n",
        "# TensorFlow in-built function for categorical crossentropy loss\n",
        "cce = tf.keras.losses.CategoricalCrossentropy()\n",
        "cce"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "LGIzrN-rPuI4"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "incomplete input (1102542339.py, line 21)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;36m  Cell \u001b[1;32mIn[24], line 21\u001b[1;36m\u001b[0m\n\u001b[1;33m    # (d) Print gradient\u001b[0m\n\u001b[1;37m                        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
          ]
        }
      ],
      "source": [
        "## Train the 0-layer neural network using batch training with batch size = 1\n",
        "\n",
        "# Steps: run over each sample, calculate loss, gradient of loss,\n",
        "# and update weights.\n",
        "\n",
        "# Step-1: add the bias feature to all the samples\n",
        "# Step-2: initialize the entries of the weights matrix randomly\n",
        "# Step-3: create softmax layer object softmax\n",
        "\n",
        "# Step-4: run over each sample\n",
        "for i in range(X.shape[1]):\n",
        "  # Step-5: forward step\n",
        "  # (a) Raw scores z = Wx = np.dot(W, x[:, i])\n",
        "  # (b) Softmax activation: softmax.forward(z)\n",
        "  # (c) Calculate cce loss for sample: cce(y[i, :], softmax.output)\n",
        "  # (d) Print cce loss\n",
        "\n",
        "  # Step-6: backward step\n",
        "  # (a) Calculate the gradient of the sample loss w.r.t. input of the\n",
        "  # softmax layer: softmax.backward(output_gradient = cce_gradient(y[i, :], softmax.output))\n",
        "  # (d) Print gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Calculate raw scores vector for a generic sample $\\mathbf{x}$  (bias feature added): $$\\mathbf{z} = \\mathbf{Wx}.$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = tf.constant([[1, 2, 3], [4, 5, 6]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "W = tf.Variable(tf.random.normal([3, 2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[-3.89800286, -6.52875423, -9.15950561],\n",
              "       [-4.53273789, -5.65191492, -6.77109195],\n",
              "       [-1.38968897, -2.18854482, -2.98740068]])"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "z = np.dot(W, x)\n",
        "z"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Calculate softmax probabilities (that is, softmax-activate the raw scores) $$\\mathbf{a} = \\text{softmax}(\\mathbf{z})\\Rightarrow\\begin{bmatrix}a_0\\\\a_1\\\\a_2\\end{bmatrix}= \\text{softmax}\\left(\\begin{bmatrix}z_0\\\\z_1\\\\z_2\\end{bmatrix}\\right)=\\begin{bmatrix}\\frac{e^{z_0}}{e^{z_0}+e^{z_1}+e^{z_2}}\\\\\\frac{e^{z_1}}{e^{z_0}+e^{z_1}+e^{z_2}}\\\\\\frac{e^{z_2}}{e^{z_0}+e^{z_1}+e^{z_2}}\\end{bmatrix}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
              "array([[-3.8980029, -6.528754 , -9.159506 ],\n",
              "       [-4.5327377, -5.651915 , -6.771092 ],\n",
              "       [-1.389689 , -2.1885448, -2.9874005]], dtype=float32)>"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "z = tf.matmul(W, tf.cast(x, tf.float32))\n",
        "z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[4.5181617e-02, 3.2541757e-03, 2.3437974e-04],\n",
              "       [2.3949694e-02, 7.8207320e-03, 2.5538483e-03],\n",
              "       [5.5502009e-01, 2.4967213e-01, 1.1231335e-01]], dtype=float32)"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "softmax = lambda z: np.exp(z) / np.sum(np.exp(z))\n",
        "a = softmax(z)\n",
        "a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Softmax loss for this sample is (where output label $y$ is not yet one-hot encoded)\n",
        "$$\\begin{align*}L &=  -\\log([a]_y) \\\\&= -\\log\\left(\\left[\\text{softmax}(\\mathbf{z})\\right]_y\\right)\\\\ &= -\\log\\left(\\left[\\text{softmax}(\\mathbf{Wx})\\right]_y\\right).\\end{align*}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3.7317996 4.850977  5.970154 ]\n"
          ]
        }
      ],
      "source": [
        "def softmax_loss(y, a):\n",
        "    return -np.log(a[y])\n",
        "\n",
        "y = 1  \n",
        "a = softmax(z)\n",
        "loss = softmax_loss(y, a)\n",
        "print(loss)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. Predicted probability vector that the sample belongs to each one of the output categories is given a new name $$\\hat{\\mathbf{y}} = \\mathbf{a}.$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[4.5181617e-02, 3.2541757e-03, 2.3437974e-04],\n",
              "       [2.3949694e-02, 7.8207320e-03, 2.5538483e-03],\n",
              "       [5.5502009e-01, 2.4967213e-01, 1.1231335e-01]], dtype=float32)"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_hat = a\n",
        "y_hat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5. One-hot encoding the output label $$\\underbrace{y\\rightarrow\\mathbf{y}}_{\\text{e.g.}\\,2\\,\\rightarrow\\begin{bmatrix}0\\\\0\\\\1\\end{bmatrix}}$$ results in the following representation for the softmax loss for the sample which is also referred to as the categorical crossentropy (CCE) loss:\n",
        "$$\\begin{align*}L &= L\\left(\\mathbf{y},\\hat{\\mathbf{y}}\\right)=\\sum_{k=0}^2-y_k\\log\\left(\\hat{y}_k\\right)\\end{align*}.$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0., 1., 0.], dtype=float32)"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_encoded = tf.keras.utils.to_categorical(y, num_labels)\n",
        "y_encoded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6. Calculate the gradient of the loss for the sample w.r.t. weights by following the computation graph from top to bottom (that is, backward):\n",
        "$$\\begin{align*} L\\\\{\\color{yellow}\\downarrow}\\\\ \\hat{\\mathbf{y}} &= \\mathbf{a}\\\\{\\color{yellow}\\downarrow}\\\\\\mathbf{z}\\\\{\\color{yellow}\\downarrow}\\\\\\mathbf{W}\\end{align*}$$\n",
        "$$\\begin{align*}\\Rightarrow \\nabla_\\mathbf{W}(L) &= \\nabla_\\mathbf{W}(\\mathbf{z}) \\times\\nabla_\\mathbf{z}(\\mathbf{a})\\times\\nabla_\\mathbf{a}(L)\\\\&= \\underbrace{\\nabla_\\mathbf{W}(\\mathbf{z})}_\\text{first term} \\times\\underbrace{\\nabla_\\mathbf{z}(\\mathbf{a})}_\\text{second to last term}\\times\\underbrace{\\nabla_\\hat{\\mathbf{y}}(L)}_\\text{last term}.\\end{align*}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.54602814 0.92693996 0.66335014 0.63450074]\n",
            "------\n",
            "------\n",
            "[[ 2.46704345e-02  4.18806464e-02  2.99712322e-02  2.86677695e-02]\n",
            " [ 1.77687149e-03  3.01642545e-03  2.15865789e-03  2.06477686e-03]\n",
            " [ 1.27977933e-04  2.17255946e-04  1.55475833e-04  1.48714117e-04]\n",
            " [-5.32950942e-01 -9.04740042e-01 -6.47463118e-01 -6.19304648e-01]\n",
            " [-5.41757807e-01 -9.19690617e-01 -6.58162264e-01 -6.29538483e-01]\n",
            " [-5.44633685e-01 -9.24572722e-01 -6.61656067e-01 -6.32880338e-01]\n",
            " [ 3.03056591e-01  5.14470304e-01  3.68172659e-01  3.52160660e-01]\n",
            " [ 1.36328009e-01  2.31431074e-01  1.65620043e-01  1.58417151e-01]\n",
            " [ 6.13262513e-02  1.04107734e-01  7.45030786e-02  7.12629052e-02]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def softmax(z):\n",
        "    exp_z = np.exp(z)\n",
        "    return exp_z / np.sum(exp_z)\n",
        "\n",
        "def softmax_loss(y, a):\n",
        "    return -np.log(a[y])\n",
        "\n",
        "def gradient_W(X, y, a):\n",
        "    # Compute the gradient of the loss with respect to z\n",
        "    grad_z = a\n",
        "    grad_z[y] -= 1\n",
        "\n",
        "    # Compute the gradient of z with respect to W\n",
        "    grad_W = np.outer(grad_z, X)\n",
        "\n",
        "    return grad_W\n",
        "\n",
        "x_0 = np.random.rand()\n",
        "x_1 = np.random.rand()\n",
        "x_2 = np.random.rand()\n",
        "x_3 = np.random.rand()\n",
        "\n",
        "X = np.array([x_0, x_1, x_2, x_3])\n",
        "\n",
        "print(X)\n",
        "print('------')\n",
        "grad_W = gradient_W(X, y, a)\n",
        "print('------')\n",
        "print(grad_W)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "7. Now focus on the last term $\\nabla_\\hat{\\mathbf{y}}(L)$:\n",
        "$$\\begin{align*}\\nabla_\\hat{\\mathbf{y}}(L) &=\\begin{bmatrix}\\nabla_{\\hat{y}_0}(L)\\\\\\nabla_{\\hat{y}_1}(L)\\\\\\nabla_{\\hat{y}_2}(L)\\end{bmatrix} = \\begin{bmatrix}-y_0/\\hat{y}_0\\\\-y_1/\\hat{y}_2\\\\-y_0/\\hat{y}_2.\\end{bmatrix}\\end{align*}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[  -0.       , -307.2975   ,   -0.       ],\n",
              "       [   0.       ,    1.0078824,    0.       ],\n",
              "       [  -0.       ,   -4.005253 ,   -0.       ]], dtype=float32)"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def gradient_y(y, y_hat):\n",
        "    grad_y = -y / y_hat\n",
        "    return grad_y\n",
        "\n",
        "grad_y = gradient_y(y_encoded, y_hat)\n",
        "grad_y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "8. Now focus on the second to last term $\\nabla_\\mathbf{z}(\\mathbf{a})$:\n",
        "$$\\begin{align*}\\nabla_\\mathbf{z}(\\mathbf{a}) &= \\nabla_\\mathbf{z}\\left(\\begin{bmatrix}a_0\\\\a_1\\\\a_2\\end{bmatrix}\\right)\\\\ &= \\begin{bmatrix}\\nabla_\\mathbf{z}(a_0)&\\nabla_\\mathbf{z}(a_1)&\\nabla_\\mathbf{z}(a_2)\\end{bmatrix} \\\\&= \\begin{bmatrix}\\nabla_{z_0}(a_0)&\\nabla_{z_0}(a_1)&\\nabla_{z_0}(a_2)\\\\\\nabla_{z_1}(a_0)&\\nabla_{z_1}(a_1)&\\nabla_{z_1}(a_2)\\\\\\nabla_{z_2}(a_0)&\\nabla_{z_2}(a_1)&\\nabla_{z_2}(a_2)\\end{bmatrix}\\\\&=\\begin{bmatrix}a_0(1-a_0)&-a_1a_0&-a_2a_0\\\\-a_0a_1&a_1(1-a_1)&-a_1a_1\\\\-a_0a_2&-a_1a_2&a_2(1-a_2)\\end{bmatrix}.\\end{align*}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 0.24334719, -0.12014334, -0.1270957 ],\n",
              "       [-0.12014334,  0.16390844, -0.04514758],\n",
              "       [-0.1270957 , -0.04514758,  0.17078079]])"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def gradient_z(a):\n",
        "    diag_a = np.diag(a)\n",
        "    return diag_a - np.outer(a, a)\n",
        "\n",
        "a_0 = np.random.rand()\n",
        "a_1 = np.random.rand()  \n",
        "a_2 = np.random.rand()\n",
        "\n",
        "a = np.array([a_0, a_1, a_2])\n",
        "grad_z = gradient_z(a)\n",
        "grad_z\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Step-1: add the bias feature to all the samples\n",
        "\n",
        "X = np.reshape(X, (X.shape[0], 1))\n",
        "X_bias = np.hstack((np.ones((X.shape[0], 1)), X))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 0.91021523 -1.00386618 -0.49760738 -0.97989465]\n",
            " [ 0.10668154  0.53053453 -1.27946603  1.16003424]\n",
            " [-0.47184029  0.76798295 -1.36321658 -1.08615807]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Step-2: initialize the entries of the weights matrix randomly\n",
        "initialize_weights = lambda num_features, num_labels: np.random.randn(num_labels, num_features)\n",
        "\n",
        "num_features = 4\n",
        "num_labels = 3\n",
        "W = initialize_weights(num_features, num_labels)\n",
        "print(W)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step-3: create softmax layer object softmax\n",
        "class SoftmaxLayer:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "    def forward(self, z):\n",
        "        exp_z = np.exp(z)\n",
        "        self.output = exp_z / np.sum(exp_z)\n",
        "    \n",
        "    def backward(self, y):\n",
        "        return self.output - y\n",
        "\n",
        "softmax = SoftmaxLayer()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss for sample 0: 4.33989799221477\n",
            "Gradient for sample 0: [-0.88190171 -0.26915383 -0.84894446]\n"
          ]
        }
      ],
      "source": [
        "def cross_entropy_loss(y_true, y_pred):\n",
        "  return -np.sum(y_true * np.log(y_pred))\n",
        "\n",
        "# Start the loop over each sample\n",
        "for i in range(X.shape[1]):\n",
        "  # Calculate the raw scores by multiplying the weights with the sample\n",
        "  z = np.dot(W, X[:, i])\n",
        "  \n",
        "  # Apply the softmax activation function\n",
        "  softmax.forward(z)\n",
        "  \n",
        "  # Calculate the cross-entropy loss for the sample\n",
        "  def cross_entropy_loss(y_true, y_pred):\n",
        "    return -np.sum(y_true * np.log(y_pred))\n",
        "  # Print the loss for the sample\n",
        "  print(f\"Loss for sample {i}: {loss}\")\n",
        "\n",
        "  # Calculate the gradient of the loss with respect to the input of the softmax layer\n",
        "  grad_z = softmax.backward(y)\n",
        "  \n",
        "  # Print the gradient for the sample\n",
        "  print(f\"Gradient for sample {i}: {grad_z}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
